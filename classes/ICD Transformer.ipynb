{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWtgAgrqIpr_"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7rqWTHsIpr_"
   },
   "source": [
    "In 2017, the Google Research team published a paper called \"Attention Is All You Need\", which presented the Transformer architecture and was a paradigm shift in Machine Learning, especially in Deep Learning and the field of natural language processing.\n",
    "\n",
    "The Transformer, with its parallel processing capabilities, allowed for more efficient and scalable models, making it easier to train them on large datasets. It also demonstrated superior performance in several NLP tasks, such as sentiment analysis and text generation tasks.\n",
    "\n",
    "The archicture presented in this paper served as the foundation for subsequent models like GPT and BERT. Besides NLP, the Transformer architecture is used in other fields, like audio processing and computer vision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:36.343359Z",
     "iopub.status.busy": "2024-01-02T20:36:36.342995Z",
     "iopub.status.idle": "2024-01-02T20:36:50.854538Z",
     "shell.execute_reply": "2024-01-02T20:36:50.853734Z",
     "shell.execute_reply.started": "2024-01-02T20:36:36.343328Z"
    },
    "id": "qBfbv-nLIpr_",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing lib: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# HuggingFace libraries\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordLevel\n",
      "File \u001b[1;32mc:\\Users\\Taiko\\miniconda3\\envs\\torch\\lib\\site-packages\\datasets\\__init__.py:24\u001b[0m\n\u001b[0;32m     20\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.12.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplatform\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(platform\u001b[38;5;241m.\u001b[39mpython_version()) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.7\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Taiko\\miniconda3\\envs\\torch\\lib\\site-packages\\pyarrow\\__init__.py:65\u001b[0m\n\u001b[0;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[0;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[0;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing lib: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import math\n",
    "\n",
    "# HuggingFace libraries\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMU-WnEYIpsA"
   },
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.856809Z",
     "iopub.status.busy": "2024-01-02T20:36:50.856257Z",
     "iopub.status.idle": "2024-01-02T20:36:50.862511Z",
     "shell.execute_reply": "2024-01-02T20:36:50.861634Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.856779Z"
    },
    "id": "GMpQNeXUIpsB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimension of vectors\n",
    "        self.vocab_size = vocab_size # Size of the vocabulary\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # PyTorch layer that converts integer indices to dense embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) # Normalizing the variance of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.863866Z",
     "iopub.status.busy": "2024-01-02T20:36:50.863608Z",
     "iopub.status.idle": "2024-01-02T20:36:50.898626Z",
     "shell.execute_reply": "2024-01-02T20:36:50.897813Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.863843Z"
    },
    "id": "43BvdldlIpsC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimensionality of the model\n",
    "        self.seq_len = seq_len # Maximum sequence length\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer to prevent overfitting\n",
    "\n",
    "        # Creating a positional encoding matrix of shape (seq_len, d_model) filled with zeros\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        # Creating a tensor representing positions (0 to seq_len - 1)\n",
    "        position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # Transforming 'position' into a 2D tensor['seq_len, 1']\n",
    "\n",
    "        # Creating the division term for the positional encoding formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sine to even indices in pe\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices in pe\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Adding an extra dimension at the beginning of pe matrix for batch handling\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Registering 'pe' as buffer. Buffer is a tensor not considered as a model parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Addind positional encoding to the input tensor X\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x) # Dropout for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.899897Z",
     "iopub.status.busy": "2024-01-02T20:36:50.899651Z",
     "iopub.status.idle": "2024-01-02T20:36:50.913256Z",
     "shell.execute_reply": "2024-01-02T20:36:50.912406Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.899875Z"
    },
    "id": "q_1386zpIpsC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float=10**-6) -> None: # We define epsilon as 0.000001 to avoid division by zero\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "        # We define alpha as a trainable parameter and initialize it with ones\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # One-dimensional tensor that will be used to scale the input data\n",
    "\n",
    "        # We define bias as a trainable parameter and initialize it with zeros\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # One-dimensional tenso that will be added to the input data\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n",
    "        std = x.std(dim = -1, keepdim = True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n",
    "\n",
    "        # Returning the normalized input\n",
    "        return self.alpha * (x-mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.917136Z",
     "iopub.status.busy": "2024-01-02T20:36:50.916274Z",
     "iopub.status.idle": "2024-01-02T20:36:50.925393Z",
     "shell.execute_reply": "2024-01-02T20:36:50.924633Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.917111Z"
    },
    "id": "OMh_tnldIpsD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # First linear transformation\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # W1 & b1\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout to prevent overfitting\n",
    "        # Second linear transformation\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 & b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff) -->(batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.926761Z",
     "iopub.status.busy": "2024-01-02T20:36:50.926477Z",
     "iopub.status.idle": "2024-01-02T20:36:50.941032Z",
     "shell.execute_reply": "2024-01-02T20:36:50.94017Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.926733Z"
    },
    "id": "o3jGXSAXIpsD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None: # h = number of heads\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        # We ensure that the dimensions of the model is divisible by the number of heads\n",
    "        assert d_model % h == 0, 'd_model is not divisible by h'\n",
    "\n",
    "        # d_k is the dimension of each attention head's key, query, and value vectors\n",
    "        self.d_k = d_model // h # d_k formula, like in the original \"Attention Is All You Need\" paper\n",
    "\n",
    "        # Defining the weight matrices\n",
    "        self.w_q = nn.Linear(d_model, d_model) # W_q\n",
    "        self.w_k = nn.Linear(d_model, d_model) # W_k\n",
    "        self.w_v = nn.Linear(d_model, d_model) # W_v\n",
    "        self.w_o = nn.Linear(d_model, d_model) # W_o\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer to avoid overfitting\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):# mask => When we want certain words to NOT interact with others, we \"hide\" them\n",
    "\n",
    "        d_k = query.shape[-1] # The last dimension of query, key, and value\n",
    "\n",
    "        # We calculate the Attention(Q,K,V) as in the formula in the image above\n",
    "        attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k) # @ = Matrix multiplication sign in PyTorch\n",
    "\n",
    "        # Before applying the softmax, we apply the mask to hide some interactions between words\n",
    "        if mask is not None: # If a mask IS defined...\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9) # Replace each value where mask is equal to 0 by -1e9\n",
    "        attention_scores = attention_scores.softmax(dim = -1) # Applying softmax\n",
    "        if dropout is not None: # If a dropout IS defined...\n",
    "            attention_scores = dropout(attention_scores) # We apply dropout to prevent overfitting\n",
    "\n",
    "        return (attention_scores @ value), attention_scores # Multiply the output matrix by the V matrix, as in the formula\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "\n",
    "        query = self.w_q(q) # Q' matrix\n",
    "        key = self.w_k(k) # K' matrix\n",
    "        value = self.w_v(v) # V' matrix\n",
    "\n",
    "\n",
    "        # Splitting results into smaller matrices for the different heads\n",
    "        # Splitting embeddings (third dimension) into h parts\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "\n",
    "        # Obtaining the output and the attention scores\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Obtaining the H matrix\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        return self.w_o(x) # Multiply the H matrix by the weight matrix W_o, resulting in the MH-A matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.94258Z",
     "iopub.status.busy": "2024-01-02T20:36:50.94223Z",
     "iopub.status.idle": "2024-01-02T20:36:50.953107Z",
     "shell.execute_reply": "2024-01-02T20:36:50.952378Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.94255Z"
    },
    "id": "thCkZGn1IpsE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout) # We use a dropout layer to prevent overfitting\n",
    "        self.norm = LayerNormalization() # We use a normalization layer\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # We normalize the input and add it to the original input 'x'. This creates the residual connection process.\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.95451Z",
     "iopub.status.busy": "2024-01-02T20:36:50.954206Z",
     "iopub.status.idle": "2024-01-02T20:36:50.963332Z",
     "shell.execute_reply": "2024-01-02T20:36:50.962602Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.954486Z"
    },
    "id": "fNGiNSqcIpsH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    # This block takes in the MultiHeadAttentionBlock and FeedForwardBlock, as well as the dropout rate for the residual connections\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # Storing the self-attention block and feed-forward block\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)]) # 2 Residual Connections with dropout\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # Applying the first residual connection with the self-attention block\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask)) # Three 'x's corresponding to query, key, and value inputs plus source mask\n",
    "\n",
    "        # Applying the second residual connection with the feed-forward block\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x # Output tensor after applying self-attention and feed-forward layers with residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.964937Z",
     "iopub.status.busy": "2024-01-02T20:36:50.964616Z",
     "iopub.status.idle": "2024-01-02T20:36:50.975372Z",
     "shell.execute_reply": "2024-01-02T20:36:50.974667Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.964907Z"
    },
    "id": "bTx0LVjTIpsH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # The Encoder takes in instances of 'EncoderBlock'\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers # Storing the EncoderBlocks\n",
    "        self.norm = LayerNormalization() # Layer for the normalization of the output of the encoder layers\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Iterating over each EncoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask) # Applying each EncoderBlock to the input tensor 'x'\n",
    "        return self.norm(x) # Normalizing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.97669Z",
     "iopub.status.busy": "2024-01-02T20:36:50.97639Z",
     "iopub.status.idle": "2024-01-02T20:36:50.986064Z",
     "shell.execute_reply": "2024-01-02T20:36:50.985169Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.97666Z"
    },
    "id": "PK8pRm0EIpsI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    # The DecoderBlock takes in two MultiHeadAttentionBlock. One is self-attention, while the other is cross-attention.\n",
    "    # It also takes in the feed-forward block and the dropout rate\n",
    "    def __init__(self,  self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)]) # List of three Residual Connections with dropout rate\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "\n",
    "        # Self-Attention block with query, key, and value plus the target language mask\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "\n",
    "        # The Cross-Attention block using two 'encoder_ouput's for key and value plus the source language mask. It also takes in 'x' for Decoder queries\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "\n",
    "        # Feed-forward block with residual connections\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.987518Z",
     "iopub.status.busy": "2024-01-02T20:36:50.987163Z",
     "iopub.status.idle": "2024-01-02T20:36:50.999092Z",
     "shell.execute_reply": "2024-01-02T20:36:50.998284Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.987485Z"
    },
    "id": "-2qTYM_AIpsI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    # The Decoder takes in instances of 'DecoderBlock'\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Storing the 'DecoderBlock's\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization() # Layer to normalize the output\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "\n",
    "        # Iterating over each DecoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            # Applies each DecoderBlock to the input 'x' plus the encoder output and source and target masks\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x) # Returns normalized output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.000452Z",
     "iopub.status.busy": "2024-01-02T20:36:51.000101Z",
     "iopub.status.idle": "2024-01-02T20:36:51.012223Z",
     "shell.execute_reply": "2024-01-02T20:36:51.011491Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.000401Z"
    },
    "id": "DXvifLheIpsJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None: # Model dimension and the size of the output vocabulary\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size) # Linear layer for projecting the feature space of 'd_model' to the output space of 'vocab_size'\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim = -1) # Applying the log Softmax function to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.013415Z",
     "iopub.status.busy": "2024-01-02T20:36:51.013184Z",
     "iopub.status.idle": "2024-01-02T20:36:51.023254Z",
     "shell.execute_reply": "2024-01-02T20:36:51.022387Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.013396Z"
    },
    "id": "v0Np5Yf2IpsJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # This takes in the encoder and decoder, as well the embeddings for the source and target language.\n",
    "    # It also takes in the Positional Encoding for the source and target language, as well as the projection layer\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    # Encoder\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src) # Applying source embeddings to the input source language\n",
    "        src = self.src_pos(src) # Applying source positional encoding to the source embeddings\n",
    "        return self.encoder(src, src_mask) # Returning the source embeddings plus a source mask to prevent attention to certain elements\n",
    "\n",
    "    # Decoder\n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt) # Applying target embeddings to the input target language (tgt)\n",
    "        tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target embeddings\n",
    "\n",
    "        # Returning the target embeddings, the output of the encoder, and both source and target masks\n",
    "        # The target mask ensures that the model won't 'see' future elements of the sequence\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "    # Applying Projection Layer with the Softmax function to the Decoder output\n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.027324Z",
     "iopub.status.busy": "2024-01-02T20:36:51.027063Z",
     "iopub.status.idle": "2024-01-02T20:36:51.038681Z",
     "shell.execute_reply": "2024-01-02T20:36:51.037833Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.027301Z"
    },
    "id": "Anio5BmHIpsJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Building & Initializing Transformer\n",
    "# Definin function and its parameter, including model dimension, number of encoder and decoder stacks, heads, etc.\n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    # Creating Embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size) # Source language (Source Vocabulary to 512-dimensional vectors)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size) # Target language (Target Vocabulary to 512-dimensional vectors)\n",
    "\n",
    "    # Creating Positional Encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout) # Positional encoding for the source language embeddings\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) # Positional encoding for the target language embeddings\n",
    "\n",
    "    # Creating EncoderBlocks\n",
    "    encoder_blocks = [] # Initial list of empty EncoderBlocks\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' EncoderBlocks (N = 6)\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "\n",
    "        # Combine layers into an EncoderBlock\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block) # Appending EncoderBlock to the list of EncoderBlocks\n",
    "\n",
    "    # Creating DecoderBlocks\n",
    "    decoder_blocks = [] # Initial list of empty DecoderBlocks\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' DecoderBlocks (N = 6)\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Cross-Attention\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "\n",
    "        # Combining layers into a DecoderBlock\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block) # Appending DecoderBlock to the list of DecoderBlocks\n",
    "\n",
    "    # Creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks lists\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # Creating projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size) # Map the output of Decoder to the Target Vocabulary Space\n",
    "\n",
    "    # Creating the transformer by combining everything above\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer # Assembled and initialized Transformer. Ready to be trained and validated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.040009Z",
     "iopub.status.busy": "2024-01-02T20:36:51.039737Z",
     "iopub.status.idle": "2024-01-02T20:36:51.051932Z",
     "shell.execute_reply": "2024-01-02T20:36:51.050996Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.039986Z"
    },
    "id": "KB59MMOOIpsK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_tokenizer(config, ds, lang):\n",
    "    # Crating a file path for the tokenizer\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "\n",
    "    # Checking if Tokenizer already exists\n",
    "    if not Path.exists(tokenizer_path):\n",
    "\n",
    "        # If it doesn't exist, we create a new one\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) # Initializing a new world-level tokenizer\n",
    "        tokenizer.pre_tokenizer = Whitespace() # We will split the text into tokens based on whitespace\n",
    "\n",
    "        # Creating a trainer for the new tokenizer\n",
    "        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\",\n",
    "                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2) # Defining Word Level strategy and special tokens\n",
    "\n",
    "        # Training new tokenizer on sentences from the dataset and language specified\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n",
    "        tokenizer.save(str(tokenizer_path)) # Saving trained tokenizer to the file path specified at the beginning of the function\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path)) # If the tokenizer already exist, we load it\n",
    "    return tokenizer # Returns the loaded tokenizer or the trained tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.066598Z",
     "iopub.status.busy": "2024-01-02T20:36:51.066241Z",
     "iopub.status.idle": "2024-01-02T20:36:51.077642Z",
     "shell.execute_reply": "2024-01-02T20:36:51.076775Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.066568Z"
    },
    "id": "kp5n0q8mIpsL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_ds(config):\n",
    "    # Loading the train portion of the OpusBooks dataset.\n",
    "    # The Language pairs will be defined in the 'config' dictionary we will build later\n",
    "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split = 'train')\n",
    "\n",
    "    # Building or loading tokenizer for both the source and target languages\n",
    "    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    # Splitting the dataset for training and validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw)) # 90% for training\n",
    "    val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n",
    "\n",
    "    # Processing data with the BilingualDataset class, which we will define below\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    for pair in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "\n",
    "    # Creating dataloaders for the training and validadion sets\n",
    "    # Dataloaders are used to iterate over the dataset in batches during training and validation\n",
    "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n",
    "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.07905Z",
     "iopub.status.busy": "2024-01-02T20:36:51.07862Z",
     "iopub.status.idle": "2024-01-02T20:36:51.08964Z",
     "shell.execute_reply": "2024-01-02T20:36:51.088945Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.079025Z"
    },
    "id": "Go3KeG5yIpsL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def casual_mask(size):\n",
    "        # Creating a square matrix of dimensions 'size x size' filled with ones\n",
    "        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
    "        return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.091366Z",
     "iopub.status.busy": "2024-01-02T20:36:51.090927Z",
     "iopub.status.idle": "2024-01-02T20:36:51.107901Z",
     "shell.execute_reply": "2024-01-02T20:36:51.107005Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.091334Z"
    },
    "id": "du3osqymIpsL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n",
    "    # 'seq_len' defines the sequence length for both languages\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        # Defining special tokens by using the target language tokenizer\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "        new_ds = []\n",
    "        for pair in ds:\n",
    "            src_text = pair['translation'][src_lang]\n",
    "            tgt_text = pair['translation'][tgt_lang]\n",
    "            \n",
    "            # Tokenizar las oraciones fuente y objetivo\n",
    "            enc_input_tokens = tokenizer_src.encode(src_text).ids\n",
    "            dec_input_tokens = tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "            # Verificar si la longitud de los tokens es v√°lida\n",
    "            if len(enc_input_tokens) + 2 <= seq_len and len(dec_input_tokens) + 1 <= seq_len:\n",
    "                new_ds.append(pair)\n",
    "        self.ds = new_ds\n",
    "\n",
    "\n",
    "    # Total number of instances in the dataset (some pairs are larger than others)\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    # Using the index to retrive source and target texts\n",
    "    def __getitem__(self, index: Any) -> Any:\n",
    "        src_target_pair = self.ds[index]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Tokenizing source and target texts\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Computing how many padding tokens need to be added to the tokenized texts\n",
    "        # Source tokens\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n",
    "        # Target tokens\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n",
    "\n",
    "        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n",
    "        # given the current sequence length limit (this will be defined in the config dictionary below)\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError('Sentence is too long')\n",
    "\n",
    "        # Building the encoder input tensor by combining several elements\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "            self.sos_token, # inserting the '[SOS]' token\n",
    "            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n",
    "            self.eos_token, # Inserting the '[EOS]' token\n",
    "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Building the decoder input tensor by combining several elements\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token, # inserting the '[SOS]' token\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "\n",
    "        )\n",
    "\n",
    "        # Creating a label tensor, the expected output for training the model\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                self.eos_token, # Inserting the '[EOS]' token\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n",
    "\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input,\n",
    "            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n",
    "            'label': label,\n",
    "            'src_text': src_text,\n",
    "            'tgt_text': tgt_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.109597Z",
     "iopub.status.busy": "2024-01-02T20:36:51.109036Z",
     "iopub.status.idle": "2024-01-02T20:36:51.120634Z",
     "shell.execute_reply": "2024-01-02T20:36:51.119875Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.109566Z"
    },
    "id": "7VXVVfdAIpsL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define function to obtain the most probable next token\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    # Retrieving the indices from the start and end of sequences of the target tokens\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Computing the output of the encoder for the source sequence\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initializing the decoder input with the Start of Sentence token\n",
    "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "    # Looping until the 'max_len', maximum length, is reached\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # Building a mask for the decoder input\n",
    "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # Calculating the output of the decoder\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # Applying the projection layer to get the probabilities for the next token\n",
    "        prob = model.project(out[:, -1])\n",
    "\n",
    "        # Selecting token with the highest probability\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "\n",
    "        # If the next token is an End of Sentence token, we finish the loop\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.121851Z",
     "iopub.status.busy": "2024-01-02T20:36:51.121624Z",
     "iopub.status.idle": "2024-01-02T20:36:51.131203Z",
     "shell.execute_reply": "2024-01-02T20:36:51.130487Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.121831Z"
    },
    "id": "bniBEhn9IpsL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Defining function to evaluate the model on the validation dataset\n",
    "# num_examples = 2, two examples per run\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
    "    model.eval() # Setting model to evaluation mode\n",
    "    count = 0 # Initializing counter to keep track of how many examples have been processed\n",
    "\n",
    "    console_width = 80 # Fixed witdh for printed messages\n",
    "\n",
    "    # Creating evaluation loop\n",
    "    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "\n",
    "            # Ensuring that the batch_size of the validation set is 1\n",
    "            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n",
    "\n",
    "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            # Retrieving source and target texts from the batch\n",
    "            source_text = batch['src_text'][0]\n",
    "            target_text = batch['tgt_text'][0] # True translation\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
    "\n",
    "            # Printing results\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f'SOURCE: {source_text}')\n",
    "            print_msg(f'TARGET: {target_text}')\n",
    "            print_msg(f'PREDICTED: {model_out_text}')\n",
    "\n",
    "            # After two examples, we break the loop\n",
    "            if count == num_examples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.132402Z",
     "iopub.status.busy": "2024-01-02T20:36:51.132141Z",
     "iopub.status.idle": "2024-01-02T20:36:51.142697Z",
     "shell.execute_reply": "2024-01-02T20:36:51.141939Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.132373Z"
    },
    "id": "LXWjsr2dIpsM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# We pass as parameters the config dictionary, the length of the vocabylary of the source language and the target language\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "\n",
    "    # Loading model using the 'build_transformer' function.\n",
    "    # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.14412Z",
     "iopub.status.busy": "2024-01-02T20:36:51.143844Z",
     "iopub.status.idle": "2024-01-02T20:36:51.155792Z",
     "shell.execute_reply": "2024-01-02T20:36:51.154977Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.144089Z"
    },
    "id": "pLPDfmnMIpsM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define settings for building and training the transformer model\n",
    "def get_config():\n",
    "    return{\n",
    "        'batch_size': 256,\n",
    "        'num_epochs': 100,\n",
    "        'lr': 10**-4,\n",
    "        'seq_len': 16,\n",
    "        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n",
    "        'lang_src': 'en',\n",
    "        'lang_tgt': 'es',\n",
    "        'model_folder': 'weights',\n",
    "        'model_basename': 'tmodel_',\n",
    "        'preload': None,\n",
    "        'tokenizer_file': 'tokenizer_{0}.json',\n",
    "        'experiment_name': 'runs/tmodel'\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to construct the path for saving and retrieving model weights\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = config['model_folder'] # Extracting model folder from the config\n",
    "    model_basename = config['model_basename'] # Extracting the base name for model files\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n",
    "    return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.157331Z",
     "iopub.status.busy": "2024-01-02T20:36:51.15705Z",
     "iopub.status.idle": "2024-01-02T20:36:51.173861Z",
     "shell.execute_reply": "2024-01-02T20:36:51.173051Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.157308Z"
    },
    "id": "k-qQdPi9IpsM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Setting up device to run on GPU to train faster\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device {device}\")\n",
    "\n",
    "    # Creating model directory to store weights\n",
    "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "\n",
    "    # Initializing model on the GPU using the 'get_model' function\n",
    "    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    # Setting up the Adam optimizer with the specified learning rate from the '\n",
    "    # config' dictionary plus an epsilon value\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n",
    "\n",
    "    # Initializing epoch and global step variables\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "    # Checking if there is a pre-trained model to load\n",
    "    # If true, loads it\n",
    "    if config['preload']:\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename) # Loading model\n",
    "\n",
    "        # Sets epoch to the saved in the state plus one, to resume from where it stopped\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        # Loading the optimizer state from the saved model\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        # Loading the global step state from the saved model\n",
    "        global_step = state['global_step']\n",
    "\n",
    "    # Initializing CrossEntropyLoss function for training\n",
    "    # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n",
    "    # We also apply label_smoothing to prevent overfitting\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n",
    "\n",
    "    # Initializing training loop\n",
    "\n",
    "    # Iterating over each epoch from the 'initial_epoch' variable up to\n",
    "    # the number of epochs informed in the config\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "\n",
    "        # Initializing an iterator over the training dataloader\n",
    "        # We also use tqdm to display a progress bar\n",
    "        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n",
    "\n",
    "        # For each batch...\n",
    "        for batch in batch_iterator:\n",
    "            model.train() # Train the model\n",
    "\n",
    "            # Loading input data and masks onto the GPU\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)\n",
    "\n",
    "            # Running tensors through the Transformer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output)\n",
    "\n",
    "            # Loading the target labels onto the GPU\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            # Computing loss between model's output and true labels\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "\n",
    "            # Updating progress bar\n",
    "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Performing backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters based on the gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clearing the gradients to prepare for the next batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            global_step += 1 # Updating global step count\n",
    "\n",
    "        # We run the 'run_validation' function at the end of each epoch\n",
    "        # to evaluate model performance\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        # Saving model\n",
    "        #model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "        # Writting current model state to the 'model_filename'\n",
    "    \"\"\"     torch.save({\n",
    "            'epoch': epoch, # Current epoch\n",
    "            'model_state_dict': model.state_dict(),# Current model state\n",
    "            'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n",
    "            'global_step': global_step # Current global step\n",
    "        }, model_filename) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.175093Z",
     "iopub.status.busy": "2024-01-02T20:36:51.17486Z",
     "iopub.status.idle": "2024-01-03T01:49:02.984508Z",
     "shell.execute_reply": "2024-01-03T01:49:02.983463Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.175072Z"
    },
    "id": "OIIpZbU5IpsN",
    "outputId": "402585b9-9b02-4033-f0e6-61efe266ce7f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Max length of source sentence: 767\n",
      "Max length of target sentence: 782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.77it/s, loss=6.533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: All day, just like the peasants?'\n",
      "TARGET: ¬øIgual que ellos? ¬øTodo el d√≠a?\n",
      "PREDICTED: ¬ø , , , , , , , , .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Never mind him.\n",
      "TARGET: ‚Äì‚ÄìNo se preocupe por √©l.\n",
      "PREDICTED: ¬ø , , , , , .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.71it/s, loss=5.981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I will answer for Ayrton's fidelity.\"\n",
      "TARGET: Yo respondo de la fidelidad de Ayrton.\n",
      "PREDICTED: ¬° No , !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: No! a duel is unthinkable and no one expects it of me.\n",
      "TARGET: El duelo es inadmisible y nadie espere que yo lo provoque.\n",
      "PREDICTED: ¬° No , la la la la la la la la la la la .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.74it/s, loss=5.695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Well, I'll come with you. May I?'\n",
      "TARGET: ‚Äì¬øPuedo acompa√±arte?\n",
      "PREDICTED: ¬ø Qu√© qu√© ? ‚Äì pregunt√≥ el .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I held the chronometer.\n",
      "TARGET: Yo ten√≠a el cron√≥metro.\n",
      "PREDICTED: ¬° No , no !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 03: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.68it/s, loss=5.436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"What!\n",
      "TARGET: ¬ª-¬øC√≥mo?\n",
      "PREDICTED: ¬° Qu√© !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"No.\"\n",
      "TARGET: No respondi√≥ Gualterio Ralph.\n",
      "PREDICTED: - No .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 04: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.59it/s, loss=5.097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Everybody knows her and Aline Stahl.'\n",
      "TARGET: A ella y a Alina Stal todos los conocen.\n",
      "PREDICTED: El y se .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: For what purpose was this meeting?\n",
      "TARGET: ¬øY por qu√© aquel mitin?\n",
      "PREDICTED: ¬ø Qu√© ha sido a la hombre ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 05: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.57it/s, loss=4.746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: It pains me to be misjudged by so good a woman.\"\n",
      "TARGET: Me disgusta que una mujer tan bondadosa como ella me juzgue mal.\n",
      "PREDICTED: ¬° a a que me !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Bogs make queer noises sometimes.\n",
      "TARGET: -Las ci√©nagas hacen a veces ruidos extra√±os.\n",
      "PREDICTED: - La hombre de la hombre de la .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 06: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.66it/s, loss=4.743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And with my own!\" the harpooner replied simply.\n",
      "TARGET: -Y la m√≠a -respondi√≥ el arponero, con la mayor simplicidad.\n",
      "PREDICTED: - Y , a mi capit√°n Nemo - dijo el capit√°n Nemo .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But the ship didn't stay long in these heavily traveled waterways.\n",
      "TARGET: Pero no permaneci√≥ por mucho tiempo en esos parajes tan frecuentados.\n",
      "PREDICTED: Pero no hab√≠a sido una vez m√°s que no hab√≠a sido de .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 07: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.64it/s, loss=4.578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But Katavasov liked the third, an artilleryman, very much.\n",
      "TARGET: En cambio el artillero despert√≥ la simpat√≠a de Katavasov.\n",
      "PREDICTED: Pero el capit√°n Nemo se hab√≠a sido un momento , pero se hab√≠a sido .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: There is not enough for three.\"\n",
      "TARGET: Es poco para tres...\n",
      "PREDICTED: No me .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 08: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.59it/s, loss=4.594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Certainly, Conseil.\n",
      "TARGET: -Claro que s√≠, Conseil.\n",
      "PREDICTED: - S√≠ , se√±or Holmes .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Why are you so glum?'\n",
      "TARGET: Pero ¬øqu√© te pasa? ¬øEst√°s triste?\n",
      "PREDICTED: ¬ø Por qu√© ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 09: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:30<00:00,  3.40it/s, loss=4.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Offended me!\n",
      "TARGET: -¬°Ofenderme!\n",
      "PREDICTED: ¬° !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: All within Elinor's breast was satisfaction, silent and strong.\n",
      "TARGET: Todo lo que abrigaba el pecho de Elinor era satisfacci√≥n, callada y fuerte.\n",
      "PREDICTED: El capit√°n Nemo se hab√≠a sido en la cabeza y se levant√≥ .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:30<00:00,  3.36it/s, loss=4.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Here's a remarkable book!\n",
      "TARGET: ‚Äî¬°Qu√© libro!\n",
      "PREDICTED: Es un hombre que es un hombre muy muy muy muy muy muy .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: What will become of me?'\n",
      "TARGET: ¬øQu√© va a ser de m√≠?\n",
      "PREDICTED: ¬ø Qu√© me ha sido ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.55it/s, loss=4.110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"A floating lighthouse,\" said someone next to me.\n",
      "TARGET: -Un faro flotante -dijo alguien cerca de m√≠.\n",
      "PREDICTED: El hombre se ha sido en seguida .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He soon extricated himself from their grasp.\n",
      "TARGET: El vigoroso Ayrton se desembaraz√≥ de ellos.\n",
      "PREDICTED: Me parece que se en su casa .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.63it/s, loss=3.949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But I could not remain alone for long.\n",
      "TARGET: Sin embargo, no pod√≠a permanecer m√°s de este tiempo solo.\n",
      "PREDICTED: Pero no pod√≠a ser m√°s m√°s m√°s m√°s m√°s que mucho tiempo .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But what do _you_ think?\"\n",
      "TARGET: Digo si le gustan a usted.\n",
      "PREDICTED: Pero , ¬ø qu√© te ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.70it/s, loss=3.806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: What a struggle!\n",
      "TARGET: ¬°Qu√© lucha!\n",
      "PREDICTED: ¬° Qu√© espect√°culo !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'But how will schools help?'\n",
      "TARGET: ‚Äì¬øDe qu√© pueden servir las escuelas?\n",
      "PREDICTED: ‚Äì Y ahora se trata de la isla ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.63it/s, loss=3.763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Try them, Pencroft,\" replied the engineer.\n",
      "TARGET: ‚ÄìProbemos, Pencroff ‚Äìdijo el ingeniero‚Äì.\n",
      "PREDICTED: ‚Äì , Pencroff ‚Äì contest√≥ el ingeniero .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I have brought my other son and daughter to see you.\n",
      "TARGET: He tra√≠do a mi otro hijo e hija para que se conozcan.\n",
      "PREDICTED: He visto mi vida y a mi amo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.66it/s, loss=3.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Trapped!\n",
      "TARGET: -¬°Atrapados!\n",
      "PREDICTED: -¬° !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'No, I will go through the garden.'\n",
      "TARGET: ‚ÄìNo, pasar√© por el jard√≠n.\n",
      "PREDICTED: ‚Äì No , voy a bordo del Palacio de granito .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.67it/s, loss=3.581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And an exceedingly interesting case it appears to be.\n",
      "TARGET: ‚Äì‚ÄìY parece tratarse de un caso sumamente interesante.\n",
      "PREDICTED: - Y una idea parece que parece que parece .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: And the most passionate and impossible romances occurred to Dolly's fancy.\n",
      "TARGET: Y las aventuras mis pasionales a irrealizables se presentaron a su imaginaci√≥n.\n",
      "PREDICTED: Y la m√°s m√°s que tambi√©n sus caballos , ¬ø no te ha venido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.63it/s, loss=3.454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: This was perfectly true, but we had nearly forgotten the fact.\n",
      "TARGET: Lo que era totalmente cierto, aunque casi lo hubi√©ramos olvidado.\n",
      "PREDICTED: Era evidente que no hab√≠a sido m√°s ; pero el d√≠a hab√≠a sido .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'No, I did not say so.\n",
      "TARGET: ‚ÄìNo, no lo he dicho...\n",
      "PREDICTED: ‚Äì No , no lo s√© .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.66it/s, loss=3.331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: And why, why does the Partition of Poland interest him?'\n",
      "TARGET: ¬øEn qu√© puede interesarle la divisi√≥n de Polonia?¬ª.\n",
      "PREDICTED: Y , ¬ø por qu√© es el asunto de √©l ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"How yes and no?\"\n",
      "TARGET: ¬øC√≥mo s√≠ y no?\n",
      "PREDICTED: ‚Äì‚Äì¬ø C√≥mo , no ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.64it/s, loss=3.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"This is indeed important,\" said he.\n",
      "TARGET: ‚Äì‚Äì¬°Esto s√≠ que es importante! ‚Äì‚Äìdijo.\n",
      "PREDICTED: ‚Äì‚Äì Eso es muy bien ‚Äì‚Äì dijo ‚Äì‚Äì.\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: It is surely Cyclopides.\"\n",
      "TARGET: Es sin duda un ejemplar de Cyclopides.\n",
      "PREDICTED: ¬° Es una !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.63it/s, loss=3.143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Love, probably!\n",
      "TARGET: Seguramente su alegr√≠a tendr√° por causa el amor.\n",
      "PREDICTED: ‚Äì Pues bien , s√≠ ...\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"I do.\"\n",
      "TARGET: -S√≠.\n",
      "PREDICTED: - S√≠ .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.62it/s, loss=3.131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: May thy joys be all dreams,\n",
      "TARGET: en sue√±os tus pasatiempos,\n",
      "PREDICTED: Si t√∫ te ,\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: We must cast round for another scent.\"\n",
      "TARGET: Hemos de seguir buscando.\n",
      "PREDICTED: No hab√≠a m√°s que un paso de hielo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.65it/s, loss=3.071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"They repent!\" exclaimed the sailor, shrugging his shoulders.\n",
      "TARGET: ‚Äì¬°Arrepentirse! ‚Äìexclam√≥ el marino encogi√©ndose de hombros.\n",
      "PREDICTED: ‚Äì¬° ! ‚Äì exclam√≥ el marino , encogi√©ndose de hombros .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'She says that?' he cried.\n",
      "TARGET: ‚Äì¬°Conque dice eso! ‚Äìexclam√≥‚Äì.\n",
      "PREDICTED: ‚Äì¬° Qu√© dice ! ‚Äì exclam√≥ .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.68it/s, loss=2.948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: It was only too true.\n",
      "TARGET: Era cierto.\n",
      "PREDICTED: No era demasiado cosa .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And pray why do you put your admiral to death?\"\n",
      "TARGET: --¬øY porqu√© han muerto √° ese almirante?\n",
      "PREDICTED: Y ahora , ¬° qu√© la muerte !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.65it/s, loss=2.920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: See how he is throwing out air and water through his blowers.\"\n",
      "TARGET: ¬°Mira el aire y el agua que arroja por las narices!\n",
      "PREDICTED: ¬ø C√≥mo sabe usted que el Sherlock Holmes se llama el agua ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I don't act and I worry.\n",
      "TARGET: Yo, al no hacer nada, me atormento.\n",
      "PREDICTED: No me pregunto si me .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.60it/s, loss=2.859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Cruel, cruel deserter!\n",
      "TARGET: -¬°Qu√© cruel fuiste, Jane!\n",
      "PREDICTED: -¬° Cruel , !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"But what hour is it, then?\" the Canadian asked.\n",
      "TARGET: -Pero ¬øqu√© hora es? -pregunt√≥ el canadiense.\n",
      "PREDICTED: - Pero , ¬ø qu√© es eso , el canadiense ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.66it/s, loss=2.763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Still, jealousy is a strange transformer of characters.\n",
      "TARGET: ‚Äì‚ÄìNo obstante, los celos pueden provocar extra√±os cambios en el car√°cter.\n",
      "PREDICTED: Sin embargo , hay celos , su situaci√≥n est√° muy satisfecho .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Anyhow, he never got tallow-stains from a gas-jet.\n",
      "TARGET: En cualquier caso, un aplique de gas no produce manchas de sebo.\n",
      "PREDICTED: M√°s que no nos da un asunto para el .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.64it/s, loss=2.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"We will make bellows of them!\"\n",
      "TARGET: Haremos de ellas fuelles de fragua.\n",
      "PREDICTED: ¬° Nos !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Ah! It is you, Treville.\n",
      "TARGET: ¬°Ay sois vos, Tr√©ville!\n",
      "PREDICTED: ¬° Ah ! ¬° Sois vos , Tr√©ville !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.67it/s, loss=2.613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"‚ÄôJudge not rashly‚Äô, says the Gospel,\" replied the cardinal.\n",
      "TARGET: ¬°No juzgu√©is temerariamente!, dice el Evangelio replic√≥ el cardenal.\n",
      "PREDICTED: ¬° no debe ser la raz√≥n ! dijo el cardenal .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Had he said too much?\n",
      "TARGET: ¬øHabr√≠a hablado demasiado?\n",
      "PREDICTED: ¬ø Hab√≠a comprendido as√≠ ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.67it/s, loss=2.569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: What do you deduce from that?\"\n",
      "TARGET: ¬øQu√© deduce usted de eso?\n",
      "PREDICTED: ¬ø Qu√© sab√©is lo de eso ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Was there a ship at our disposal in some underground harbour?\n",
      "TARGET: ¬øHab√≠a fondeado un buque en alg√∫n puerto interior?\n",
      "PREDICTED: ¬ø Hab√≠a alg√∫n buque a su viaje sobre sus papeles ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.64it/s, loss=2.503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: If the current was interrupted, the magnet immediately became unmagnetized.\n",
      "TARGET: Si la corriente se interrump√≠a, el electroim√°n se desimantaba inmediatamente.\n",
      "PREDICTED: Si la corriente , Ned , se hallaba profundamente delante de √©l .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: His finger pulled the trigger before he had taken aim.\n",
      "TARGET: Su dedo oprim√≠a el gatillo antes de apuntar bien.\n",
      "PREDICTED: El dedo sac√≥ una se√±a del cad√°ver .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.63it/s, loss=2.390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Papa says I mustn't.\n",
      "TARGET: ¬øHas visto a Basilio Lukich?\n",
      "PREDICTED: pap√° me parece que no .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Our General stood in need of new recruits of young German Jesuits.\n",
      "TARGET: El padre general necesitaba una leva de jesuitas alemanes mozos.\n",
      "PREDICTED: El vapor continuaba cerca de esa nueva Ferrars y bajo el impasible .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.67it/s, loss=2.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: CHAPTER XXVI\n",
      "TARGET: Cap√≠tulo XXVI\n",
      "PREDICTED: XXVI\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: That day the usual work was accomplished with even greater energy.\n",
      "TARGET: Aquel d√≠a se realiz√≥ con m√°s vigor a√∫n el trabajo habitual.\n",
      "PREDICTED: Sin d√≠a la d√≠a se tambi√©n con el trabajo fr√≠a ; me parec√≠a .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.70it/s, loss=2.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"No. His orders were to stay in the house.\"\n",
      "TARGET: ‚Äì‚ÄìNo, sus √≥rdenes son permanecer en la casa.\n",
      "PREDICTED: - No , su √≥rdenes cuando esperaba a la casa .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Was it Ayrton?\n",
      "TARGET: ¬øEra Ayrton?\n",
      "PREDICTED: ¬ø Era Ayrton ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.66it/s, loss=2.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Me! why to me?\"\n",
      "TARGET: ¬øA m√≠? ¬øY eso por qu√©?\n",
      "PREDICTED: ¬ø A m√≠ ? ¬ø A qu√© me ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"It is a wound that confines him to his bed?\"\n",
      "TARGET: Entonces, ¬øes una estocada lo que le retiene en su cama?\n",
      "PREDICTED: ¬ø Es un herida que le haya venido a la cama ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.65it/s, loss=2.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Mr. Conseil put one over on me!\"\n",
      "TARGET: El se√±or Conseil me estaba tomando el pelo.\n",
      "PREDICTED: -¬° Conseil ! - exclam√≥ Conseil .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: His whole being was concentrated in this last word.\n",
      "TARGET: En esa frase estaba expresado todo el arponero.\n",
      "PREDICTED: Toda su situaci√≥n estaba concentrada en el √∫ltimo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.66it/s, loss=2.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: IN WHICH THE CAPTIVE STILL CONTINUES HIS ADVENTURES\n",
      "TARGET: Donde todav√≠a prosigue el cautivo su suceso\n",
      "PREDICTED: Donde el cautivo las maravillas del cautivo\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"I will not give him that trouble,\" I answered.\n",
      "TARGET: -No tiene por qu√© molestarse tanto -dije-.\n",
      "PREDICTED: ‚Äì‚Äì No me da cuenta de esto ‚Äì‚Äì respond√≠ ‚Äì‚Äì.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.72it/s, loss=2.135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"No--stop!\" interrupted Colonel Dent.\n",
      "TARGET: -¬°No! -interrumpi√≥ el coronel Dent-.\n",
      "PREDICTED: - No , no - respondi√≥ el coronel Dent -.\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Fix, seated in the bow, gave himself up to meditation.\n",
      "TARGET: Fix estaba meditabundo en la proa.\n",
      "PREDICTED: Fix , pues , baj√≥ la campanilla y me dirigi√≥ a una especie .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.67it/s, loss=2.115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: However, do as you please!'\n",
      "TARGET: Sin embargo, haz lo que te parezca mejor.\n",
      "PREDICTED: En fin ; como al fin te lo ruego .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He positively avoids me.\"\n",
      "TARGET: Parece evitarme.\n",
      "PREDICTED: Pronto tom√≥ mi camarote .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.66it/s, loss=2.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And you too, Mr. Spilett, you will eat some!\"\n",
      "TARGET: ‚ÄìCon mucho gusto ‚Äìdijo el corresponsal‚Äì.\n",
      "PREDICTED: ‚Äì Y t√∫ , se√±or Spilett , con nosotros .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'I ought to have done that long ago.'\n",
      "TARGET: ¬°Ya pod√≠amos haberlo hecho hace tiempo!\n",
      "PREDICTED: Hace tiempo que me he pasado ya .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.63it/s, loss=2.043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Vronsky remained silent.\n",
      "TARGET: Vronsky callaba.\n",
      "PREDICTED: Vronsky callaba .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Certainly, Neb,\" answered Cyrus Harding.\n",
      "TARGET: ‚ÄìS√≠, Nab ‚Äìrepuso Ciro Smith.\n",
      "PREDICTED: ‚Äì S√≠ , se√±or ‚Äì contest√≥ Ciro Smith ‚Äì.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:29<00:00,  3.53it/s, loss=1.978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: M Bonacieux was at his door.\n",
      "TARGET: El se√±or Bonacieux estaba a su puerta.\n",
      "PREDICTED: El se√±or Bonacieux estaba en la puerta .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The man replied that the Count had gone to the stables.\n",
      "TARGET: El criado contest√≥ que el Conde se dirig√≠a a las cuadras\n",
      "PREDICTED: El hombre respondi√≥ que el se√±or conde iban juntos .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:30<00:00,  3.41it/s, loss=1.883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And who has abandoned you--is that it?\"\n",
      "TARGET: Y que os ha abandonado, ¬øno es eso?\n",
      "PREDICTED: -¬ø Y qui√©n tiene usted dinero ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I have brought my other son and daughter to see you.\n",
      "TARGET: He tra√≠do a mi otro hijo e hija para que se conozcan.\n",
      "PREDICTED: He tra√≠do mi hijo y a mi hijo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:30<00:00,  3.37it/s, loss=1.894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: It was worth while going a little hungry.\n",
      "TARGET: Vale la pena quedarse sin comer.\n",
      "PREDICTED: Su dolor pasaba un poco gigantesco .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ‚Äî Oui, je sais ce que vous avez vu.\n",
      "TARGET: ‚Äì‚ÄìS√≠, s√© ya todo lo que usted vio.\n",
      "PREDICTED: ‚Äì‚Äì Le aseguro que no es verdad .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:30<00:00,  3.39it/s, loss=1.864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Take away the prisoner,\" said the commissary to the two guards.\n",
      "TARGET: Llevaos al prisionero dijo el comisario a los dos guardias.\n",
      "PREDICTED: Tomad a vuestra prisionera dijo el comisario al comisario .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Milady smiled.\n",
      "TARGET: Milady sonri√≥.\n",
      "PREDICTED: Milady sonri√≥ .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:30<00:00,  3.40it/s, loss=1.857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Let us be off to Tver.\n",
      "TARGET: La ventana est√° abierta.\n",
      "PREDICTED: Hemos ido con la suficiente .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I am simply unhappy.\n",
      "TARGET: Soy muy desgraciada.\n",
      "PREDICTED: Estoy verdaderamente desgraciada .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:29<00:00,  3.47it/s, loss=1.801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: You are killing me now.\"\n",
      "TARGET: Ya me est√°s matando ahora.\n",
      "PREDICTED: Aqu√≠ est√°s aqu√≠ .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Not exactly. Your witch's skill is rather at fault sometimes.\"\n",
      "TARGET: No es eso precisamente.\n",
      "PREDICTED: Ni m√°s claramente el almirante de visitar a las opiniones de su doble manera .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.67it/s, loss=1.816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: How clearly he puts everything!'\n",
      "TARGET: Lo ve todo con una claridad...\n",
      "PREDICTED: ¬° Qu√© delgada y qu√© se lo pone !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"You are hungry,\" I remarked.\n",
      "TARGET: ‚Äì‚ÄìViene usted hambriento ‚Äì‚Äìcoment√©.\n",
      "PREDICTED: - Usted es - dije .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.64it/s, loss=1.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: It was forty-eight hours since I had taken any nourishment.\n",
      "TARGET: Cuando me despert√©, una nueva mesa estaba servida.\n",
      "PREDICTED: Era cuarenta y ocho horas que no hubieran hecho m√°s o cinco a√±os .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He entered the chamber and closed the door behind him.\n",
      "TARGET: Entr√≥ en la habitaci√≥n y cerr√≥ la puerta tras s√≠.\n",
      "PREDICTED: Entr√≥ en la habitaci√≥n y la puerta sali√≥ de la puerta .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.55it/s, loss=1.745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Levin felt guilty but could do nothing.\n",
      "TARGET: Levin, sin poderlo remediar, se sent√≠a culpable.\n",
      "PREDICTED: Levin sent√≠a culpable y no pod√≠a hacer nada .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: CHAPTER XVIII\n",
      "TARGET: Cap√≠tulo XVIII\n",
      "PREDICTED: XVIII\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:30<00:00,  3.39it/s, loss=1.725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Kitty, don't be angry!\n",
      "TARGET: ‚ÄìKitty, no te enfades.\n",
      "PREDICTED: ‚Äì¬ø C√≥mo , Kitty ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'What's the matter?'\n",
      "TARGET: ‚Äì¬øQu√© te pasa?\n",
      "PREDICTED: ‚Äì¬ø Qu√© te pasa ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:29<00:00,  3.50it/s, loss=1.700]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I am simply unhappy.\n",
      "TARGET: Soy muy desgraciada.\n",
      "PREDICTED: Estoy incapaz .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: What will he say when he returns?\"\n",
      "TARGET: ¬øQu√© dir√° cuando vuelva?\n",
      "PREDICTED: ¬ø Qu√© opina usted ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.66it/s, loss=1.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"What do you mean, then?\"\n",
      "TARGET: -Entonces, ¬øa qu√© se refiere?\n",
      "PREDICTED: ‚Äì‚Äì¬ø Qu√© quiere usted decir ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"No!\" replies my uncle.\n",
      "TARGET: ‚Äî¬°No! ‚Äîresponde mi t√≠o.\n",
      "PREDICTED: ‚Äî No , mi t√≠o .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.68it/s, loss=1.711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"How!\" said Bonacieux, astonished.\n",
      "TARGET: ¬øC√≥mo? dijo Bonacieux, extra√±ado.\n",
      "PREDICTED: ¬ø C√≥mo ? dijo Bonacieux asombrado .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He liked Katavasov because of his clear and simple outlook on life.\n",
      "TARGET: Katavasov le atra√≠a por la claridad y sencillez de sus ideas.\n",
      "PREDICTED: Se acerc√≥ a Katavasov y tranquila , y tranquila ; pero es excelente vida .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.64it/s, loss=1.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"I have heard of you, Mr. Holmes.\n",
      "TARGET: ‚Äì‚ÄìHe o√≠do hablar de usted, se√±or Holmes.\n",
      "PREDICTED: - Lo he o√≠do , se√±or Holmes .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"You are very good.\n",
      "TARGET: -Es usted muy amable.\n",
      "PREDICTED: ‚Äì‚Äì¬ø C√≥mo est√° usted bien ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 55: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.64it/s, loss=1.657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Yes, sir.\"\n",
      "TARGET: -S√≠.\n",
      "PREDICTED: - S√≠ , se√±or .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Jane Eyre\n",
      "TARGET: Jane Eyre\n",
      "PREDICTED: Jane ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.63it/s, loss=1.649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: How the theories will hinder us, won't they?\"\n",
      "TARGET: ¬°Cu√°nto van a darnos que hacer!\n",
      "PREDICTED: ¬ø C√≥mo los gatos son estos gatos ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Es ist ein ganz einfaches Ding,' [Oh, yes!\n",
      "TARGET: Es ist ein ganz einfaches Ding.\n",
      "PREDICTED: ‚Äì Es preciso ‚Äì a√±adi√≥ Vronsky .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.69it/s, loss=1.634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: One part of Captain Nemo's secret life had been unveiled.\n",
      "TARGET: Eso desvelaba una parte de la misteriosa existencia del capit√°n Nemo.\n",
      "PREDICTED: Un poco de tierra , el capit√°n Nemo hab√≠a sido alguna .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Round the world?\" cried Fix.\n",
      "TARGET: ¬øLa vuelta al mundo? Exclam√≥ Fix.\n",
      "PREDICTED: ¬ø La vuelta al mundo ? dijo Fix .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.68it/s, loss=1.627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The company rose to go into the garden.\n",
      "TARGET: Los invitados se levantaron en aquel momento para salir al jard√≠n.\n",
      "PREDICTED: El choque tom√≥ bastante al jard√≠n .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Kindly follow this man.\"\n",
      "TARGET: Tengan la amabilidad de seguir a este hombre.\n",
      "PREDICTED: - Entonces , esta persona .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.60it/s, loss=1.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: In your place I would stake the furniture against the horse.\"\n",
      "TARGET: En vuestro lugar, yo jugar√≠a vuestros arneses contra vuestro caballo.\n",
      "PREDICTED: En vuestro sitio les sigo roto el caballo .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Why don't you like her husband?\n",
      "TARGET: ‚Äì¬øY por qu√© a su marido no?\n",
      "PREDICTED: ‚Äì¬ø C√≥mo no te gusta su marido ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.63it/s, loss=1.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"As far as the centre of the earth, Axel.\"\n",
      "TARGET: ‚ÄîS√≠, hasta el centro de la tierra.\n",
      "PREDICTED: ‚Äî Tan s√≥lo el centro de la tierra .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: His finger pulled the trigger before he had taken aim.\n",
      "TARGET: Su dedo oprim√≠a el gatillo antes de apuntar bien.\n",
      "PREDICTED: Sus manos se tras haber marchado como el horizonte .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.69it/s, loss=1.590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I inquired soon if he had not been to London.\n",
      "TARGET: Le pregunt√© si hab√≠a estado en Londres.\n",
      "PREDICTED: Pregunt√© por ti si no se hubiera dicho nada .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Yes, when the house where he fraternizes is suspected.\"\n",
      "TARGET: S√≠, cuando la casa en la que confraterniza con ese amigo es sospechosa.\n",
      "PREDICTED: - S√≠ , cuando el resto de la casa .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.69it/s, loss=1.608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Why _did_ Mr. Rochester enforce this concealment?\n",
      "TARGET: ¬øPor qu√© Rochester toleraba aquello?\n",
      "PREDICTED: ¬ø Y qu√© hizo entonces los turcos ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'He is pitiable, he is overwhelmed with remorse...'\n",
      "TARGET: Es digno tambi√©n de compasi√≥n; el arrepentimiento le tiene abatido.\n",
      "PREDICTED: Se trata de un paseo como cerrado por remordimientos .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 63: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.62it/s, loss=1.580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Drink and relate, then.\"\n",
      "TARGET: Bebed y contad.\n",
      "PREDICTED: - Bebe y maquinalmente .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The atmosphere was turning white and milky.\n",
      "TARGET: La atm√≥sfera estaba blanca, lechosa.\n",
      "PREDICTED: Hac√≠a una escena que salieron bajo los planes y la capa de espesor .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.65it/s, loss=1.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'But what was there in church on Sunday?\n",
      "TARGET: ‚Äì¬øQu√© pas√≥ el domingo en la iglesia? ‚Äìpregunt√≥ el Pr√≠ncipe‚Äì.\n",
      "PREDICTED: ‚Äì Pero ¬ø qu√© era en una iglesia ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Was our presence on board perhaps a burden to him?\n",
      "TARGET: ¬øTal vez se le hac√≠a insoportable nuestra presencia a bordo?\n",
      "PREDICTED: ¬ø Era nuestra presencia un detalle como yo ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 65: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.68it/s, loss=1.558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Everybody knew your errand.\"\n",
      "TARGET: Todos lo saben.\n",
      "PREDICTED: Todos lo dem√°s se que es capaz de enamorarse .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I didn't tell my two companions about this new danger.\n",
      "TARGET: Me abstuve de comunicar este nuevo peligro a mis dos compa√±eros.\n",
      "PREDICTED: No pude responderle entre mis dos compa√±eros es √©ste .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 66: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.68it/s, loss=1.573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And you said nothing?\"\n",
      "TARGET: -¬øY no dijo nada?\n",
      "PREDICTED: ¬ø Y no hab√©is dicho nada ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: During the last week of the month of August the weather moderated again.\n",
      "TARGET: Durante la √∫ltima semana de aquel mes de agosto el tiempo cambi√≥ de nuevo.\n",
      "PREDICTED: Durante la semana de las siete de roca desnuda de .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.70it/s, loss=1.545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"What about master's live babirusa?\"\n",
      "TARGET: -¬øY el babirusa vivo del se√±or?\n",
      "PREDICTED: -¬ø Y el se√±or de los \" No es all√° ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I may then tell the cardinal, with respect to this little woman--\"\n",
      "TARGET: Puedo, por tanto, decir al cardenal que, respecto a esa mujer...\n",
      "PREDICTED: S√© que el cardenal le a su mujer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 68: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.70it/s, loss=1.554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Nothing.\"\n",
      "TARGET: ‚Äì¬°S√≠!...\n",
      "PREDICTED: ‚Äì‚Äì Nada .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Then she broke the silence to cry out,\n",
      "TARGET: Entonces ella rompi√≥ el silencio para exclamar:\n",
      "PREDICTED: Luego , dio el silencio de cabeza un instante , cruz√≥ la mente .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 69: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.70it/s, loss=1.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I am staying there while I conduct the inquiry.\"\n",
      "TARGET: Clair. Me estoy alojando all√≠ mientras llevo a cabo la investigaci√≥n.\n",
      "PREDICTED: aqu√≠ por buen camino ; supongo que la investigaci√≥n .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"When will we reach Vanikoro?\"\n",
      "TARGET: -¬øCu√°ndo estaremos en Vanikoro?\n",
      "PREDICTED: -¬ø Cu√°ndo nos vamos a Vanikoro ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 70: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.68it/s, loss=1.538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"I have won five pistoles of Aramis.\"\n",
      "TARGET: Le he ganado cinco pistolas a Aramis.\n",
      "PREDICTED: Tengo cinco pistolas .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Signed, Richard Mason.'\"\n",
      "TARGET: Firmado: Richard Mason.¬ª\n",
      "PREDICTED: : ¬° C√≥mo te llamas !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 71: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.68it/s, loss=1.555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: That happened which M. de Treville had foreseen.\n",
      "TARGET: Lo que hab√≠a previsto el se√±or de Tr√©ville ocurri√≥.\n",
      "PREDICTED: De maravilla que la misma del se√±or Tr√©ville hab√≠a contado encima .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'I never maintained it,' thought Levin...\n",
      "TARGET: ¬´Jam√°s lo he asegurado¬ª, pens√≥ Levin.\n",
      "PREDICTED: ¬´¬° No he pensado nada ! ‚Äì coment√≥ Levin ‚Äì.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 72: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.70it/s, loss=1.530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"The Miss Reeds could not play as well!\" said she exultingly.\n",
      "TARGET: -¬°Las se√±oritas no tocan tan bien! -dijo con entusiasmo-.\n",
      "PREDICTED: -¬° La se√±orita Stoner permit√≠a desplazarse como no ! - dijo con voz baja .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Our case is not complete.\n",
      "TARGET: -Nuestro caso no est√° terminado.\n",
      "PREDICTED: El caso no es absolutamente nada .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.70it/s, loss=1.535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: No wind, and not a cloud in the sky.\n",
      "TARGET: No hay viento, ni se ve una nube en el cielo.\n",
      "PREDICTED: El viento y media absoluta ventana estaba vac√≠a y los colonos .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: You should come and join us here.'\n",
      "TARGET: Porque en este caso podr√≠a sentarse con nosotros.\n",
      "PREDICTED: Usted volver√° a tu parte .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 74: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.74it/s, loss=1.541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Gregor had almost entirely stopped eating.\n",
      "TARGET: Gregorio ya no com√≠a casi nada.\n",
      "PREDICTED: Gregorio todos estaban √∫nicamente para tomar siquiera .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Well, shall we go?'\n",
      "TARGET: ¬øVamos?\n",
      "PREDICTED: ‚Äì¬ø Qu√© ? ¬ø Vamos ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 75: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.70it/s, loss=1.531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Yes, yes, yes...'\n",
      "TARGET: ‚ÄìS√≠, s√≠, s√≠.\n",
      "PREDICTED: ‚Äì S√≠ , s√≠ ...\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"But suppose it is an extinct volcano?\"\n",
      "TARGET: ‚Äî¬øY si se trata de un cr√°ter apagado?\n",
      "PREDICTED: ‚Äì¬ø Y eso est√° cerrado ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 76: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.69it/s, loss=1.519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'I shall be very glad,' replied Koznyshev, still smiling.\n",
      "TARGET: ‚ÄìConforme. Me gustar√° mucho ‚Äìcontest√≥ Sergio Ivanovich, siempre sonriente.\n",
      "PREDICTED: ‚Äì Lo har√© muy simp√°tica ‚Äì dijo Sergio , sonriendo .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: This current swept along with it a whole host of moving creatures.\n",
      "TARGET: La corriente arrastraba con ella a todo un mundo de seres vivos.\n",
      "PREDICTED: Esta corriente , se por una persona una verdadera debilidad de nuestro viaje .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 77: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.70it/s, loss=1.527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Have I anything to pay?\" demanded d‚ÄôArtagnan.\n",
      "TARGET: ¬øDebo algo? pregunt√≥ D'Artagnan.\n",
      "PREDICTED: ¬ø Es algo que hacer ? pregunt√≥ D ' Artagnan .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Certainly, my best.\n",
      "TARGET: -S√≠, querida.\n",
      "PREDICTED: - Desde luego , ser√≠a lo mejor .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.70it/s, loss=1.510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Oh, yes, yes; you are right.\n",
      "TARGET: ¬°Oh, s√≠, s√≠, ten√©is raz√≥n!\n",
      "PREDICTED: ¬° Oh , s√≠ ! ¬ø Conque ten√©is raz√≥n ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: One is a she-bear.\n",
      "TARGET: ¬°V√°monos ahora mismo a Tver!\n",
      "PREDICTED: Hay una .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.71it/s, loss=1.504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But what kind of man was he?\n",
      "TARGET: Pero, ¬øqu√© clase de persona era?\n",
      "PREDICTED: Pero ¬ø qu√© hombre era √©l ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"No, I am descending.\"\n",
      "TARGET: ‚Äì‚ÄìEn efecto, voy descendiendo.\n",
      "PREDICTED: - No , me quedo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.69it/s, loss=1.504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Certainly, my best.\n",
      "TARGET: -S√≠, querida.\n",
      "PREDICTED: - Desde luego , es lo que necesito .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The Nautilus kept descending.\n",
      "TARGET: El Nautilus continu√≥ descendiendo.\n",
      "PREDICTED: El Nautilus parec√≠a un violento .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:28<00:00,  3.66it/s, loss=1.500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Her arms rose and her hands dropped on his shoulders.\n",
      "TARGET: Sus manos se levantaron y se posaron en los hombros de Levin.\n",
      "PREDICTED: Ana se levant√≥ y cogi√≥ la mano sobre sus hombros .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Why so?\" said Sancho.\n",
      "TARGET: -Pues, ¬øpor qu√©? -dijo Sancho.\n",
      "PREDICTED: -¬ø Por qu√© ? - dijo Sancho -.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.69it/s, loss=1.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I am staying there while I conduct the inquiry.\"\n",
      "TARGET: Clair. Me estoy alojando all√≠ mientras llevo a cabo la investigaci√≥n.\n",
      "PREDICTED: Estoy aqu√≠ para mi propia roca . Yo estoy vivo en los medios .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"With a wood round it?\"\n",
      "TARGET: -¬øY un bosque alrededor?\n",
      "PREDICTED: -¬ø Con un bosque ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 83: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.68it/s, loss=1.511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He said he would come back.'\n",
      "TARGET: Ha dicho que volver√≠a.\n",
      "PREDICTED: √âl dijo que hab√≠a llegado .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Why don't you consult my art?\"\n",
      "TARGET: -¬øC√≥mo no quer√≠a consultar mi ciencia?\n",
      "PREDICTED: -¬ø C√≥mo usted ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 84: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.77it/s, loss=1.488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"That is the exact situation.\n",
      "TARGET: -Buenos d√≠as, Holmes -dijo el baronet-.\n",
      "PREDICTED: Es la situaci√≥n de la situaci√≥n .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And does she go from Suez directly to Bombay?\"\n",
      "TARGET: ¬øY de Suez se marcha directamente a Bombay?\n",
      "PREDICTED: ¬ø Y est√° bien en Suez de este Bombay ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 85: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.79it/s, loss=1.500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Lord!\n",
      "TARGET: -¬°Por Dios, Anne! -exclam√≥ su hermana-.\n",
      "PREDICTED: ¬° Ay , Dios !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"The devil!\" muttered Passepartout.\n",
      "TARGET: ¬°Diantre! exclam√≥ Picaporte.\n",
      "PREDICTED: ¬° Diablos ! exclam√≥ Picaporte .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 86: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.81it/s, loss=1.484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Don't you see now whence these words have been taken?\"\n",
      "TARGET: ¬øVe usted ahora de d√≥nde se han tomado esas palabras?\n",
      "PREDICTED: ¬ø No va usted francamente de esos palabras ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Yes, perfectly well--intimately even.\"\n",
      "TARGET: S√≠, perfectamente, mucho incluso.\n",
      "PREDICTED: ‚Äì S√≠ , en efecto : el est√° √≠ntimamente ligado .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 87: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.81it/s, loss=1.487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: To this no reply was possible.\n",
      "TARGET: No hab√≠a respuesta posible a esa pregunta.\n",
      "PREDICTED: A ello no fue posible .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: You consider Vronsky an aristocrat. I don't.\n",
      "TARGET: T√∫ consideras que Vronsky es un arist√≥crata y yo no.\n",
      "PREDICTED: Usted ha hecho Vronsky junto a Vronsky .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 88: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.79it/s, loss=1.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: They fear them, therefore they must know them.\"\n",
      "TARGET: Los temen, luego los conocen.\n",
      "PREDICTED: No est√°n m√°s pues , como pueden saber .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Does he wish it?'\n",
      "TARGET: ¬øLo desea √©l?\n",
      "PREDICTED: ¬ø Estar√° usted satisfecho ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 89: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.78it/s, loss=1.495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: My undaunted uncle calmly shook his head.\n",
      "TARGET: Mi t√≠o sacudi√≥ la cabeza con calma.\n",
      "PREDICTED: Mi t√≠o golpe√≥ la cabeza con la cabeza .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Where?\n",
      "TARGET: ''¬øAd√≥nde?\n",
      "PREDICTED: Pero ¬ø d√≥nde ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 90: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.79it/s, loss=1.479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: How can I ask them away from her?\"\n",
      "TARGET: ¬øC√≥mo puedo pedirles que la dejen?\n",
      "PREDICTED: ¬ø C√≥mo podr√© retener de ella ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Gr√§uben was far away; and I never hoped to see her again.\n",
      "TARGET: Y ni aun esperanzas ten√≠a de volver a verla jam√°s.\n",
      "PREDICTED: Gra√ºben .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 91: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:26<00:00,  3.82it/s, loss=1.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"The Coroner: What do you mean?\n",
      "TARGET: ¬ªEl juez: ¬øQu√© quiere decir con eso?\n",
      "PREDICTED: ¬ª El juez : ¬ø Qu√© quer√©is decir ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Amen!' from the invisible choir, again floated through the air.\n",
      "TARGET: ¬´¬°Am√©n!¬ª llenaron de nuevo el aire las voces del coro.\n",
      "PREDICTED: ‚Äì Am√©n a Am√©n de paseo por los Scherbazky , se dirigi√≥ al aire .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.80it/s, loss=1.486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Karenin entered the boudoir.\n",
      "TARGET: Alexey Alejandrovich entr√≥ en el gabinete de Ana.\n",
      "PREDICTED: Alexey Alejandrovich entr√≥ con los √∫ltimos movimientos .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: CHAPTER 32\n",
      "TARGET: CAPITULO XXXII\n",
      "PREDICTED: Cap√≠tulo XXXII\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 93: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.77it/s, loss=1.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: CHAPTER XXVIII. THE RESCUE IN THE WHISPERING GALLERY\n",
      "TARGET: Cap√≠tulo XXVIII\n",
      "PREDICTED: Cap√≠tulo XXVIII\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Very ill?\n",
      "TARGET: ¬øMuy enferma?\n",
      "PREDICTED: ‚Äì Muy mal .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.78it/s, loss=1.473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"That is well.\n",
      "TARGET: ‚Äì‚ÄìEso est√° bien.\n",
      "PREDICTED: ‚Äì‚Äì Estupendo .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Speak out, man, and don't stand staring!\"\n",
      "TARGET: ¬°Hable, caramba, y no se me quede mirando!\n",
      "PREDICTED: ¬° Hablad , hombre , y no con voz alta !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.77it/s, loss=1.479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Confound it!\" exclaimed the sailor.\n",
      "TARGET: ‚Äì¬°Maldici√≥n! ‚Äìexclam√≥ el marino, sin contenerse.\n",
      "PREDICTED: ‚Äì¬° Miss Eyre ! ‚Äì exclam√≥ el marino .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"I am, then, your prisoner?\"\n",
      "TARGET: ¬øSoy, pues, vuestra prisionera?\n",
      "PREDICTED: Entonces , ¬ø os har√© vuestra prisionera ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.79it/s, loss=1.477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Alexander smiled gaily.\n",
      "TARGET: Alejandro Vronsky, que lo sab√≠a, sonri√≥ con jovialidad.\n",
      "PREDICTED: Alejandro .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He was a nobleman, a man equal to Buckingham in every respect.\n",
      "TARGET: Era un gran se√±or, era un hombre en todo el igual de Buckingham.\n",
      "PREDICTED: El noble lo comprender√° todo a su comandante Ketty .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.78it/s, loss=1.475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Impatient of delay, with reckless pace\n",
      "TARGET: Sali√≥ el deseo de comp√°s, y el paso\n",
      "PREDICTED: Cruel Vireno , con gran estruendo , , se dirigieron a sus pies .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: CHAPTER VII\n",
      "TARGET: VII\n",
      "PREDICTED: VII\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:26<00:00,  3.83it/s, loss=1.470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"What do you mean?\"\n",
      "TARGET: -¬øPor qu√©?\n",
      "PREDICTED: -¬ø Qu√© quieres decir ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Being fossils, we looked upon all those things as mere jokes.\n",
      "TARGET: En nuestra calidad de f√≥siles, nos burl√°bamos de estas maravillas in√∫tiles.\n",
      "PREDICTED: No pudiendo nosotros , los mir√≥ de una situaci√≥n , sobre las regiones .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:27<00:00,  3.79it/s, loss=1.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'I was only going to say that...'\n",
      "TARGET: ‚ÄìQuisiera decirte...\n",
      "PREDICTED: ‚Äì S√≥lo quer√≠a decir ..\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"What, when we have nothing to do but keep going down!\"\n",
      "TARGET: ‚Äî¬°C√≥mo fatigoso, cuando siempre caminamos cuesta abajo!\n",
      "PREDICTED: -¬° Qu√© , cu√°ndo no vamos a conseguir nada !\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore') # Filtering warnings\n",
    "config = get_config() # Retrieving config settings\n",
    "train_model(config) # Training model with the config arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taiko\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¬øQu√©, cuando no tenemos nada que hacer sino seguir bajando!', 'S√≥lo iba a decir que...', 'Siendo f√≥siles, ve√≠amos todas esas cosas como meras bromas.', '¬øQu√© quieres decir?']\n"
     ]
    }
   ],
   "source": [
    "src_text = ['What, when we have nothing to do but keep going down!', 'I was only going to say that...', 'Being fossils, we looked upon all those things as mere jokes.', 'What do you mean?']\n",
    "inputs = tokenizer(src_text, return_tensors='pt', padding=True)\n",
    "translated = model.generate(**inputs)\n",
    "tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "Transformer From Scratch With PyTorchüî•",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
