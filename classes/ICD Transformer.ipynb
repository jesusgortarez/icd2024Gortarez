{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWtgAgrqIpr_"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7rqWTHsIpr_"
   },
   "source": [
    "In 2017, the Google Research team published a paper called \"Attention Is All You Need\", which presented the Transformer architecture and was a paradigm shift in Machine Learning, especially in Deep Learning and the field of natural language processing.\n",
    "\n",
    "The Transformer, with its parallel processing capabilities, allowed for more efficient and scalable models, making it easier to train them on large datasets. It also demonstrated superior performance in several NLP tasks, such as sentiment analysis and text generation tasks.\n",
    "\n",
    "The archicture presented in this paper served as the foundation for subsequent models like GPT and BERT. Besides NLP, the Transformer architecture is used in other fields, like audio processing and computer vision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:36.343359Z",
     "iopub.status.busy": "2024-01-02T20:36:36.342995Z",
     "iopub.status.idle": "2024-01-02T20:36:50.854538Z",
     "shell.execute_reply": "2024-01-02T20:36:50.853734Z",
     "shell.execute_reply.started": "2024-01-02T20:36:36.343328Z"
    },
    "id": "qBfbv-nLIpr_",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing lib: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# HuggingFace libraries\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordLevel\n",
      "File \u001b[1;32mc:\\Users\\Taiko\\miniconda3\\envs\\torch\\lib\\site-packages\\datasets\\__init__.py:24\u001b[0m\n\u001b[0;32m     20\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.12.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplatform\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(platform\u001b[38;5;241m.\u001b[39mpython_version()) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.7\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Taiko\\miniconda3\\envs\\torch\\lib\\site-packages\\pyarrow\\__init__.py:65\u001b[0m\n\u001b[0;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[0;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[0;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing lib: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import math\n",
    "\n",
    "# HuggingFace libraries\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMU-WnEYIpsA"
   },
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.856809Z",
     "iopub.status.busy": "2024-01-02T20:36:50.856257Z",
     "iopub.status.idle": "2024-01-02T20:36:50.862511Z",
     "shell.execute_reply": "2024-01-02T20:36:50.861634Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.856779Z"
    },
    "id": "GMpQNeXUIpsB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimension of vectors\n",
    "        self.vocab_size = vocab_size # Size of the vocabulary\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # PyTorch layer that converts integer indices to dense embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) # Normalizing the variance of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.863866Z",
     "iopub.status.busy": "2024-01-02T20:36:50.863608Z",
     "iopub.status.idle": "2024-01-02T20:36:50.898626Z",
     "shell.execute_reply": "2024-01-02T20:36:50.897813Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.863843Z"
    },
    "id": "43BvdldlIpsC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimensionality of the model\n",
    "        self.seq_len = seq_len # Maximum sequence length\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer to prevent overfitting\n",
    "\n",
    "        # Creating a positional encoding matrix of shape (seq_len, d_model) filled with zeros\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        # Creating a tensor representing positions (0 to seq_len - 1)\n",
    "        position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # Transforming 'position' into a 2D tensor['seq_len, 1']\n",
    "\n",
    "        # Creating the division term for the positional encoding formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sine to even indices in pe\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices in pe\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Adding an extra dimension at the beginning of pe matrix for batch handling\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Registering 'pe' as buffer. Buffer is a tensor not considered as a model parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Addind positional encoding to the input tensor X\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x) # Dropout for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.899897Z",
     "iopub.status.busy": "2024-01-02T20:36:50.899651Z",
     "iopub.status.idle": "2024-01-02T20:36:50.913256Z",
     "shell.execute_reply": "2024-01-02T20:36:50.912406Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.899875Z"
    },
    "id": "q_1386zpIpsC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float=10**-6) -> None: # We define epsilon as 0.000001 to avoid division by zero\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "        # We define alpha as a trainable parameter and initialize it with ones\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # One-dimensional tensor that will be used to scale the input data\n",
    "\n",
    "        # We define bias as a trainable parameter and initialize it with zeros\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # One-dimensional tenso that will be added to the input data\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n",
    "        std = x.std(dim = -1, keepdim = True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n",
    "\n",
    "        # Returning the normalized input\n",
    "        return self.alpha * (x-mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.917136Z",
     "iopub.status.busy": "2024-01-02T20:36:50.916274Z",
     "iopub.status.idle": "2024-01-02T20:36:50.925393Z",
     "shell.execute_reply": "2024-01-02T20:36:50.924633Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.917111Z"
    },
    "id": "OMh_tnldIpsD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # First linear transformation\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # W1 & b1\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout to prevent overfitting\n",
    "        # Second linear transformation\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 & b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff) -->(batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.926761Z",
     "iopub.status.busy": "2024-01-02T20:36:50.926477Z",
     "iopub.status.idle": "2024-01-02T20:36:50.941032Z",
     "shell.execute_reply": "2024-01-02T20:36:50.94017Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.926733Z"
    },
    "id": "o3jGXSAXIpsD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None: # h = number of heads\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        # We ensure that the dimensions of the model is divisible by the number of heads\n",
    "        assert d_model % h == 0, 'd_model is not divisible by h'\n",
    "\n",
    "        # d_k is the dimension of each attention head's key, query, and value vectors\n",
    "        self.d_k = d_model // h # d_k formula, like in the original \"Attention Is All You Need\" paper\n",
    "\n",
    "        # Defining the weight matrices\n",
    "        self.w_q = nn.Linear(d_model, d_model) # W_q\n",
    "        self.w_k = nn.Linear(d_model, d_model) # W_k\n",
    "        self.w_v = nn.Linear(d_model, d_model) # W_v\n",
    "        self.w_o = nn.Linear(d_model, d_model) # W_o\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer to avoid overfitting\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):# mask => When we want certain words to NOT interact with others, we \"hide\" them\n",
    "\n",
    "        d_k = query.shape[-1] # The last dimension of query, key, and value\n",
    "\n",
    "        # We calculate the Attention(Q,K,V) as in the formula in the image above\n",
    "        attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k) # @ = Matrix multiplication sign in PyTorch\n",
    "\n",
    "        # Before applying the softmax, we apply the mask to hide some interactions between words\n",
    "        if mask is not None: # If a mask IS defined...\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9) # Replace each value where mask is equal to 0 by -1e9\n",
    "        attention_scores = attention_scores.softmax(dim = -1) # Applying softmax\n",
    "        if dropout is not None: # If a dropout IS defined...\n",
    "            attention_scores = dropout(attention_scores) # We apply dropout to prevent overfitting\n",
    "\n",
    "        return (attention_scores @ value), attention_scores # Multiply the output matrix by the V matrix, as in the formula\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "\n",
    "        query = self.w_q(q) # Q' matrix\n",
    "        key = self.w_k(k) # K' matrix\n",
    "        value = self.w_v(v) # V' matrix\n",
    "\n",
    "\n",
    "        # Splitting results into smaller matrices for the different heads\n",
    "        # Splitting embeddings (third dimension) into h parts\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "\n",
    "        # Obtaining the output and the attention scores\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Obtaining the H matrix\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        return self.w_o(x) # Multiply the H matrix by the weight matrix W_o, resulting in the MH-A matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.94258Z",
     "iopub.status.busy": "2024-01-02T20:36:50.94223Z",
     "iopub.status.idle": "2024-01-02T20:36:50.953107Z",
     "shell.execute_reply": "2024-01-02T20:36:50.952378Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.94255Z"
    },
    "id": "thCkZGn1IpsE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout) # We use a dropout layer to prevent overfitting\n",
    "        self.norm = LayerNormalization() # We use a normalization layer\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # We normalize the input and add it to the original input 'x'. This creates the residual connection process.\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.95451Z",
     "iopub.status.busy": "2024-01-02T20:36:50.954206Z",
     "iopub.status.idle": "2024-01-02T20:36:50.963332Z",
     "shell.execute_reply": "2024-01-02T20:36:50.962602Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.954486Z"
    },
    "id": "fNGiNSqcIpsH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    # This block takes in the MultiHeadAttentionBlock and FeedForwardBlock, as well as the dropout rate for the residual connections\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # Storing the self-attention block and feed-forward block\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)]) # 2 Residual Connections with dropout\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # Applying the first residual connection with the self-attention block\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask)) # Three 'x's corresponding to query, key, and value inputs plus source mask\n",
    "\n",
    "        # Applying the second residual connection with the feed-forward block\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x # Output tensor after applying self-attention and feed-forward layers with residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.964937Z",
     "iopub.status.busy": "2024-01-02T20:36:50.964616Z",
     "iopub.status.idle": "2024-01-02T20:36:50.975372Z",
     "shell.execute_reply": "2024-01-02T20:36:50.974667Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.964907Z"
    },
    "id": "bTx0LVjTIpsH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # The Encoder takes in instances of 'EncoderBlock'\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers # Storing the EncoderBlocks\n",
    "        self.norm = LayerNormalization() # Layer for the normalization of the output of the encoder layers\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Iterating over each EncoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask) # Applying each EncoderBlock to the input tensor 'x'\n",
    "        return self.norm(x) # Normalizing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.97669Z",
     "iopub.status.busy": "2024-01-02T20:36:50.97639Z",
     "iopub.status.idle": "2024-01-02T20:36:50.986064Z",
     "shell.execute_reply": "2024-01-02T20:36:50.985169Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.97666Z"
    },
    "id": "PK8pRm0EIpsI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    # The DecoderBlock takes in two MultiHeadAttentionBlock. One is self-attention, while the other is cross-attention.\n",
    "    # It also takes in the feed-forward block and the dropout rate\n",
    "    def __init__(self,  self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)]) # List of three Residual Connections with dropout rate\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "\n",
    "        # Self-Attention block with query, key, and value plus the target language mask\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "\n",
    "        # The Cross-Attention block using two 'encoder_ouput's for key and value plus the source language mask. It also takes in 'x' for Decoder queries\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "\n",
    "        # Feed-forward block with residual connections\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:50.987518Z",
     "iopub.status.busy": "2024-01-02T20:36:50.987163Z",
     "iopub.status.idle": "2024-01-02T20:36:50.999092Z",
     "shell.execute_reply": "2024-01-02T20:36:50.998284Z",
     "shell.execute_reply.started": "2024-01-02T20:36:50.987485Z"
    },
    "id": "-2qTYM_AIpsI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    # The Decoder takes in instances of 'DecoderBlock'\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Storing the 'DecoderBlock's\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization() # Layer to normalize the output\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "\n",
    "        # Iterating over each DecoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            # Applies each DecoderBlock to the input 'x' plus the encoder output and source and target masks\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x) # Returns normalized output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.000452Z",
     "iopub.status.busy": "2024-01-02T20:36:51.000101Z",
     "iopub.status.idle": "2024-01-02T20:36:51.012223Z",
     "shell.execute_reply": "2024-01-02T20:36:51.011491Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.000401Z"
    },
    "id": "DXvifLheIpsJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None: # Model dimension and the size of the output vocabulary\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size) # Linear layer for projecting the feature space of 'd_model' to the output space of 'vocab_size'\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim = -1) # Applying the log Softmax function to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.013415Z",
     "iopub.status.busy": "2024-01-02T20:36:51.013184Z",
     "iopub.status.idle": "2024-01-02T20:36:51.023254Z",
     "shell.execute_reply": "2024-01-02T20:36:51.022387Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.013396Z"
    },
    "id": "v0Np5Yf2IpsJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # This takes in the encoder and decoder, as well the embeddings for the source and target language.\n",
    "    # It also takes in the Positional Encoding for the source and target language, as well as the projection layer\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    # Encoder\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src) # Applying source embeddings to the input source language\n",
    "        src = self.src_pos(src) # Applying source positional encoding to the source embeddings\n",
    "        return self.encoder(src, src_mask) # Returning the source embeddings plus a source mask to prevent attention to certain elements\n",
    "\n",
    "    # Decoder\n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt) # Applying target embeddings to the input target language (tgt)\n",
    "        tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target embeddings\n",
    "\n",
    "        # Returning the target embeddings, the output of the encoder, and both source and target masks\n",
    "        # The target mask ensures that the model won't 'see' future elements of the sequence\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "    # Applying Projection Layer with the Softmax function to the Decoder output\n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.027324Z",
     "iopub.status.busy": "2024-01-02T20:36:51.027063Z",
     "iopub.status.idle": "2024-01-02T20:36:51.038681Z",
     "shell.execute_reply": "2024-01-02T20:36:51.037833Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.027301Z"
    },
    "id": "Anio5BmHIpsJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Building & Initializing Transformer\n",
    "# Definin function and its parameter, including model dimension, number of encoder and decoder stacks, heads, etc.\n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    # Creating Embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size) # Source language (Source Vocabulary to 512-dimensional vectors)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size) # Target language (Target Vocabulary to 512-dimensional vectors)\n",
    "\n",
    "    # Creating Positional Encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout) # Positional encoding for the source language embeddings\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) # Positional encoding for the target language embeddings\n",
    "\n",
    "    # Creating EncoderBlocks\n",
    "    encoder_blocks = [] # Initial list of empty EncoderBlocks\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' EncoderBlocks (N = 6)\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "\n",
    "        # Combine layers into an EncoderBlock\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block) # Appending EncoderBlock to the list of EncoderBlocks\n",
    "\n",
    "    # Creating DecoderBlocks\n",
    "    decoder_blocks = [] # Initial list of empty DecoderBlocks\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' DecoderBlocks (N = 6)\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Cross-Attention\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "\n",
    "        # Combining layers into a DecoderBlock\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block) # Appending DecoderBlock to the list of DecoderBlocks\n",
    "\n",
    "    # Creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks lists\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # Creating projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size) # Map the output of Decoder to the Target Vocabulary Space\n",
    "\n",
    "    # Creating the transformer by combining everything above\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer # Assembled and initialized Transformer. Ready to be trained and validated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.040009Z",
     "iopub.status.busy": "2024-01-02T20:36:51.039737Z",
     "iopub.status.idle": "2024-01-02T20:36:51.051932Z",
     "shell.execute_reply": "2024-01-02T20:36:51.050996Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.039986Z"
    },
    "id": "KB59MMOOIpsK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_tokenizer(config, ds, lang):\n",
    "    # Crating a file path for the tokenizer\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "\n",
    "    # Checking if Tokenizer already exists\n",
    "    if not Path.exists(tokenizer_path):\n",
    "\n",
    "        # If it doesn't exist, we create a new one\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) # Initializing a new world-level tokenizer\n",
    "        tokenizer.pre_tokenizer = Whitespace() # We will split the text into tokens based on whitespace\n",
    "\n",
    "        # Creating a trainer for the new tokenizer\n",
    "        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\",\n",
    "                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2) # Defining Word Level strategy and special tokens\n",
    "\n",
    "        # Training new tokenizer on sentences from the dataset and language specified\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n",
    "        tokenizer.save(str(tokenizer_path)) # Saving trained tokenizer to the file path specified at the beginning of the function\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path)) # If the tokenizer already exist, we load it\n",
    "    return tokenizer # Returns the loaded tokenizer or the trained tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.066598Z",
     "iopub.status.busy": "2024-01-02T20:36:51.066241Z",
     "iopub.status.idle": "2024-01-02T20:36:51.077642Z",
     "shell.execute_reply": "2024-01-02T20:36:51.076775Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.066568Z"
    },
    "id": "kp5n0q8mIpsL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_ds(config):\n",
    "    # Loading the train portion of the OpusBooks dataset.\n",
    "    # The Language pairs will be defined in the 'config' dictionary we will build later\n",
    "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split = 'train')\n",
    "\n",
    "    # Building or loading tokenizer for both the source and target languages\n",
    "    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    # Splitting the dataset for training and validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw)) # 90% for training\n",
    "    val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n",
    "\n",
    "    # Processing data with the BilingualDataset class, which we will define below\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    for pair in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "\n",
    "    # Creating dataloaders for the training and validadion sets\n",
    "    # Dataloaders are used to iterate over the dataset in batches during training and validation\n",
    "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n",
    "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.07905Z",
     "iopub.status.busy": "2024-01-02T20:36:51.07862Z",
     "iopub.status.idle": "2024-01-02T20:36:51.08964Z",
     "shell.execute_reply": "2024-01-02T20:36:51.088945Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.079025Z"
    },
    "id": "Go3KeG5yIpsL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def casual_mask(size):\n",
    "        # Creating a square matrix of dimensions 'size x size' filled with ones\n",
    "        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
    "        return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.091366Z",
     "iopub.status.busy": "2024-01-02T20:36:51.090927Z",
     "iopub.status.idle": "2024-01-02T20:36:51.107901Z",
     "shell.execute_reply": "2024-01-02T20:36:51.107005Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.091334Z"
    },
    "id": "du3osqymIpsL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n",
    "    # 'seq_len' defines the sequence length for both languages\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        # Defining special tokens by using the target language tokenizer\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "        new_ds = []\n",
    "        for pair in ds:\n",
    "            src_text = pair['translation'][src_lang]\n",
    "            tgt_text = pair['translation'][tgt_lang]\n",
    "            \n",
    "            # Tokenizar las oraciones fuente y objetivo\n",
    "            enc_input_tokens = tokenizer_src.encode(src_text).ids\n",
    "            dec_input_tokens = tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "            # Verificar si la longitud de los tokens es válida\n",
    "            if len(enc_input_tokens) + 2 <= seq_len and len(dec_input_tokens) + 1 <= seq_len:\n",
    "                new_ds.append(pair)\n",
    "        self.ds = new_ds\n",
    "\n",
    "\n",
    "    # Total number of instances in the dataset (some pairs are larger than others)\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    # Using the index to retrive source and target texts\n",
    "    def __getitem__(self, index: Any) -> Any:\n",
    "        src_target_pair = self.ds[index]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Tokenizing source and target texts\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Computing how many padding tokens need to be added to the tokenized texts\n",
    "        # Source tokens\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n",
    "        # Target tokens\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n",
    "\n",
    "        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n",
    "        # given the current sequence length limit (this will be defined in the config dictionary below)\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError('Sentence is too long')\n",
    "\n",
    "        # Building the encoder input tensor by combining several elements\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "            self.sos_token, # inserting the '[SOS]' token\n",
    "            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n",
    "            self.eos_token, # Inserting the '[EOS]' token\n",
    "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Building the decoder input tensor by combining several elements\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token, # inserting the '[SOS]' token\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "\n",
    "        )\n",
    "\n",
    "        # Creating a label tensor, the expected output for training the model\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                self.eos_token, # Inserting the '[EOS]' token\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n",
    "\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input,\n",
    "            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n",
    "            'label': label,\n",
    "            'src_text': src_text,\n",
    "            'tgt_text': tgt_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.109597Z",
     "iopub.status.busy": "2024-01-02T20:36:51.109036Z",
     "iopub.status.idle": "2024-01-02T20:36:51.120634Z",
     "shell.execute_reply": "2024-01-02T20:36:51.119875Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.109566Z"
    },
    "id": "7VXVVfdAIpsL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define function to obtain the most probable next token\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    # Retrieving the indices from the start and end of sequences of the target tokens\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Computing the output of the encoder for the source sequence\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initializing the decoder input with the Start of Sentence token\n",
    "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "    # Looping until the 'max_len', maximum length, is reached\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # Building a mask for the decoder input\n",
    "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # Calculating the output of the decoder\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # Applying the projection layer to get the probabilities for the next token\n",
    "        prob = model.project(out[:, -1])\n",
    "\n",
    "        # Selecting token with the highest probability\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "\n",
    "        # If the next token is an End of Sentence token, we finish the loop\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.121851Z",
     "iopub.status.busy": "2024-01-02T20:36:51.121624Z",
     "iopub.status.idle": "2024-01-02T20:36:51.131203Z",
     "shell.execute_reply": "2024-01-02T20:36:51.130487Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.121831Z"
    },
    "id": "bniBEhn9IpsL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Defining function to evaluate the model on the validation dataset\n",
    "# num_examples = 2, two examples per run\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
    "    model.eval() # Setting model to evaluation mode\n",
    "    count = 0 # Initializing counter to keep track of how many examples have been processed\n",
    "\n",
    "    console_width = 80 # Fixed witdh for printed messages\n",
    "\n",
    "    # Creating evaluation loop\n",
    "    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "\n",
    "            # Ensuring that the batch_size of the validation set is 1\n",
    "            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n",
    "\n",
    "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            # Retrieving source and target texts from the batch\n",
    "            source_text = batch['src_text'][0]\n",
    "            target_text = batch['tgt_text'][0] # True translation\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
    "\n",
    "            # Printing results\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f'SOURCE: {source_text}')\n",
    "            print_msg(f'TARGET: {target_text}')\n",
    "            print_msg(f'PREDICTED: {model_out_text}')\n",
    "\n",
    "            # After two examples, we break the loop\n",
    "            if count == num_examples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.132402Z",
     "iopub.status.busy": "2024-01-02T20:36:51.132141Z",
     "iopub.status.idle": "2024-01-02T20:36:51.142697Z",
     "shell.execute_reply": "2024-01-02T20:36:51.141939Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.132373Z"
    },
    "id": "LXWjsr2dIpsM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# We pass as parameters the config dictionary, the length of the vocabylary of the source language and the target language\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "\n",
    "    # Loading model using the 'build_transformer' function.\n",
    "    # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.14412Z",
     "iopub.status.busy": "2024-01-02T20:36:51.143844Z",
     "iopub.status.idle": "2024-01-02T20:36:51.155792Z",
     "shell.execute_reply": "2024-01-02T20:36:51.154977Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.144089Z"
    },
    "id": "pLPDfmnMIpsM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define settings for building and training the transformer model\n",
    "def get_config():\n",
    "    return{\n",
    "        'batch_size': 256,\n",
    "        'num_epochs': 100,\n",
    "        'lr': 10**-4,\n",
    "        'seq_len': 16,\n",
    "        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n",
    "        'lang_src': 'en',\n",
    "        'lang_tgt': 'es',\n",
    "        'model_folder': 'weights',\n",
    "        'model_basename': 'tmodel_',\n",
    "        'preload': None,\n",
    "        'tokenizer_file': 'tokenizer_{0}.json',\n",
    "        'experiment_name': 'runs/tmodel'\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to construct the path for saving and retrieving model weights\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = config['model_folder'] # Extracting model folder from the config\n",
    "    model_basename = config['model_basename'] # Extracting the base name for model files\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n",
    "    return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.157331Z",
     "iopub.status.busy": "2024-01-02T20:36:51.15705Z",
     "iopub.status.idle": "2024-01-02T20:36:51.173861Z",
     "shell.execute_reply": "2024-01-02T20:36:51.173051Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.157308Z"
    },
    "id": "k-qQdPi9IpsM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Setting up device to run on GPU to train faster\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device {device}\")\n",
    "\n",
    "    # Creating model directory to store weights\n",
    "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "\n",
    "    # Initializing model on the GPU using the 'get_model' function\n",
    "    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    # Setting up the Adam optimizer with the specified learning rate from the '\n",
    "    # config' dictionary plus an epsilon value\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n",
    "\n",
    "    # Initializing epoch and global step variables\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "    # Checking if there is a pre-trained model to load\n",
    "    # If true, loads it\n",
    "    if config['preload']:\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename) # Loading model\n",
    "\n",
    "        # Sets epoch to the saved in the state plus one, to resume from where it stopped\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        # Loading the optimizer state from the saved model\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        # Loading the global step state from the saved model\n",
    "        global_step = state['global_step']\n",
    "\n",
    "    # Initializing CrossEntropyLoss function for training\n",
    "    # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n",
    "    # We also apply label_smoothing to prevent overfitting\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n",
    "\n",
    "    # Initializing training loop\n",
    "\n",
    "    # Iterating over each epoch from the 'initial_epoch' variable up to\n",
    "    # the number of epochs informed in the config\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "\n",
    "        # Initializing an iterator over the training dataloader\n",
    "        # We also use tqdm to display a progress bar\n",
    "        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n",
    "\n",
    "        # For each batch...\n",
    "        for batch in batch_iterator:\n",
    "            model.train() # Train the model\n",
    "\n",
    "            # Loading input data and masks onto the GPU\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)\n",
    "\n",
    "            # Running tensors through the Transformer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output)\n",
    "\n",
    "            # Loading the target labels onto the GPU\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            # Computing loss between model's output and true labels\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "\n",
    "            # Updating progress bar\n",
    "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Performing backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters based on the gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clearing the gradients to prepare for the next batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            global_step += 1 # Updating global step count\n",
    "\n",
    "        # We run the 'run_validation' function at the end of each epoch\n",
    "        # to evaluate model performance\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        # Saving model\n",
    "        #model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "        # Writting current model state to the 'model_filename'\n",
    "    \"\"\"     torch.save({\n",
    "            'epoch': epoch, # Current epoch\n",
    "            'model_state_dict': model.state_dict(),# Current model state\n",
    "            'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n",
    "            'global_step': global_step # Current global step\n",
    "        }, model_filename) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-01-02T20:36:51.175093Z",
     "iopub.status.busy": "2024-01-02T20:36:51.17486Z",
     "iopub.status.idle": "2024-01-03T01:49:02.984508Z",
     "shell.execute_reply": "2024-01-03T01:49:02.983463Z",
     "shell.execute_reply.started": "2024-01-02T20:36:51.175072Z"
    },
    "id": "OIIpZbU5IpsN",
    "outputId": "402585b9-9b02-4033-f0e6-61efe266ce7f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Max length of source sentence: 767\n",
      "Max length of target sentence: 782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 00: 100%|██████████| 103/103 [00:27<00:00,  3.77it/s, loss=6.533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: All day, just like the peasants?'\n",
      "TARGET: ¿Igual que ellos? ¿Todo el día?\n",
      "PREDICTED: ¿ , , , , , , , , .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Never mind him.\n",
      "TARGET: ––No se preocupe por él.\n",
      "PREDICTED: ¿ , , , , , .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 01: 100%|██████████| 103/103 [00:27<00:00,  3.71it/s, loss=5.981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I will answer for Ayrton's fidelity.\"\n",
      "TARGET: Yo respondo de la fidelidad de Ayrton.\n",
      "PREDICTED: ¡ No , !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: No! a duel is unthinkable and no one expects it of me.\n",
      "TARGET: El duelo es inadmisible y nadie espere que yo lo provoque.\n",
      "PREDICTED: ¡ No , la la la la la la la la la la la .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 02: 100%|██████████| 103/103 [00:27<00:00,  3.74it/s, loss=5.695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Well, I'll come with you. May I?'\n",
      "TARGET: –¿Puedo acompañarte?\n",
      "PREDICTED: ¿ Qué qué ? – preguntó el .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I held the chronometer.\n",
      "TARGET: Yo tenía el cronómetro.\n",
      "PREDICTED: ¡ No , no !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 03: 100%|██████████| 103/103 [00:27<00:00,  3.68it/s, loss=5.436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"What!\n",
      "TARGET: »-¿Cómo?\n",
      "PREDICTED: ¡ Qué !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"No.\"\n",
      "TARGET: No respondió Gualterio Ralph.\n",
      "PREDICTED: - No .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 04: 100%|██████████| 103/103 [00:28<00:00,  3.59it/s, loss=5.097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Everybody knows her and Aline Stahl.'\n",
      "TARGET: A ella y a Alina Stal todos los conocen.\n",
      "PREDICTED: El y se .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: For what purpose was this meeting?\n",
      "TARGET: ¿Y por qué aquel mitin?\n",
      "PREDICTED: ¿ Qué ha sido a la hombre ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 05: 100%|██████████| 103/103 [00:28<00:00,  3.57it/s, loss=4.746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: It pains me to be misjudged by so good a woman.\"\n",
      "TARGET: Me disgusta que una mujer tan bondadosa como ella me juzgue mal.\n",
      "PREDICTED: ¡ a a que me !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Bogs make queer noises sometimes.\n",
      "TARGET: -Las ciénagas hacen a veces ruidos extraños.\n",
      "PREDICTED: - La hombre de la hombre de la .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 06: 100%|██████████| 103/103 [00:28<00:00,  3.66it/s, loss=4.743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And with my own!\" the harpooner replied simply.\n",
      "TARGET: -Y la mía -respondió el arponero, con la mayor simplicidad.\n",
      "PREDICTED: - Y , a mi capitán Nemo - dijo el capitán Nemo .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But the ship didn't stay long in these heavily traveled waterways.\n",
      "TARGET: Pero no permaneció por mucho tiempo en esos parajes tan frecuentados.\n",
      "PREDICTED: Pero no había sido una vez más que no había sido de .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 07: 100%|██████████| 103/103 [00:28<00:00,  3.64it/s, loss=4.578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But Katavasov liked the third, an artilleryman, very much.\n",
      "TARGET: En cambio el artillero despertó la simpatía de Katavasov.\n",
      "PREDICTED: Pero el capitán Nemo se había sido un momento , pero se había sido .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: There is not enough for three.\"\n",
      "TARGET: Es poco para tres...\n",
      "PREDICTED: No me .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 08: 100%|██████████| 103/103 [00:28<00:00,  3.59it/s, loss=4.594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Certainly, Conseil.\n",
      "TARGET: -Claro que sí, Conseil.\n",
      "PREDICTED: - Sí , señor Holmes .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Why are you so glum?'\n",
      "TARGET: Pero ¿qué te pasa? ¿Estás triste?\n",
      "PREDICTED: ¿ Por qué ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 09: 100%|██████████| 103/103 [00:30<00:00,  3.40it/s, loss=4.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Offended me!\n",
      "TARGET: -¡Ofenderme!\n",
      "PREDICTED: ¡ !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: All within Elinor's breast was satisfaction, silent and strong.\n",
      "TARGET: Todo lo que abrigaba el pecho de Elinor era satisfacción, callada y fuerte.\n",
      "PREDICTED: El capitán Nemo se había sido en la cabeza y se levantó .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 10: 100%|██████████| 103/103 [00:30<00:00,  3.36it/s, loss=4.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Here's a remarkable book!\n",
      "TARGET: —¡Qué libro!\n",
      "PREDICTED: Es un hombre que es un hombre muy muy muy muy muy muy .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: What will become of me?'\n",
      "TARGET: ¿Qué va a ser de mí?\n",
      "PREDICTED: ¿ Qué me ha sido ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 11: 100%|██████████| 103/103 [00:28<00:00,  3.55it/s, loss=4.110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"A floating lighthouse,\" said someone next to me.\n",
      "TARGET: -Un faro flotante -dijo alguien cerca de mí.\n",
      "PREDICTED: El hombre se ha sido en seguida .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He soon extricated himself from their grasp.\n",
      "TARGET: El vigoroso Ayrton se desembarazó de ellos.\n",
      "PREDICTED: Me parece que se en su casa .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 12: 100%|██████████| 103/103 [00:28<00:00,  3.63it/s, loss=3.949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But I could not remain alone for long.\n",
      "TARGET: Sin embargo, no podía permanecer más de este tiempo solo.\n",
      "PREDICTED: Pero no podía ser más más más más más que mucho tiempo .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But what do _you_ think?\"\n",
      "TARGET: Digo si le gustan a usted.\n",
      "PREDICTED: Pero , ¿ qué te ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 13: 100%|██████████| 103/103 [00:27<00:00,  3.70it/s, loss=3.806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: What a struggle!\n",
      "TARGET: ¡Qué lucha!\n",
      "PREDICTED: ¡ Qué espectáculo !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'But how will schools help?'\n",
      "TARGET: –¿De qué pueden servir las escuelas?\n",
      "PREDICTED: – Y ahora se trata de la isla ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 14: 100%|██████████| 103/103 [00:28<00:00,  3.63it/s, loss=3.763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Try them, Pencroft,\" replied the engineer.\n",
      "TARGET: –Probemos, Pencroff –dijo el ingeniero–.\n",
      "PREDICTED: – , Pencroff – contestó el ingeniero .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I have brought my other son and daughter to see you.\n",
      "TARGET: He traído a mi otro hijo e hija para que se conozcan.\n",
      "PREDICTED: He visto mi vida y a mi amo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 15: 100%|██████████| 103/103 [00:28<00:00,  3.66it/s, loss=3.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Trapped!\n",
      "TARGET: -¡Atrapados!\n",
      "PREDICTED: -¡ !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'No, I will go through the garden.'\n",
      "TARGET: –No, pasaré por el jardín.\n",
      "PREDICTED: – No , voy a bordo del Palacio de granito .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 16: 100%|██████████| 103/103 [00:28<00:00,  3.67it/s, loss=3.581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And an exceedingly interesting case it appears to be.\n",
      "TARGET: ––Y parece tratarse de un caso sumamente interesante.\n",
      "PREDICTED: - Y una idea parece que parece que parece .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: And the most passionate and impossible romances occurred to Dolly's fancy.\n",
      "TARGET: Y las aventuras mis pasionales a irrealizables se presentaron a su imaginación.\n",
      "PREDICTED: Y la más más que también sus caballos , ¿ no te ha venido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 17: 100%|██████████| 103/103 [00:28<00:00,  3.63it/s, loss=3.454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: This was perfectly true, but we had nearly forgotten the fact.\n",
      "TARGET: Lo que era totalmente cierto, aunque casi lo hubiéramos olvidado.\n",
      "PREDICTED: Era evidente que no había sido más ; pero el día había sido .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'No, I did not say so.\n",
      "TARGET: –No, no lo he dicho...\n",
      "PREDICTED: – No , no lo sé .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 18: 100%|██████████| 103/103 [00:28<00:00,  3.66it/s, loss=3.331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: And why, why does the Partition of Poland interest him?'\n",
      "TARGET: ¿En qué puede interesarle la división de Polonia?».\n",
      "PREDICTED: Y , ¿ por qué es el asunto de él ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"How yes and no?\"\n",
      "TARGET: ¿Cómo sí y no?\n",
      "PREDICTED: ––¿ Cómo , no ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 19: 100%|██████████| 103/103 [00:28<00:00,  3.64it/s, loss=3.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"This is indeed important,\" said he.\n",
      "TARGET: ––¡Esto sí que es importante! ––dijo.\n",
      "PREDICTED: –– Eso es muy bien –– dijo ––.\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: It is surely Cyclopides.\"\n",
      "TARGET: Es sin duda un ejemplar de Cyclopides.\n",
      "PREDICTED: ¡ Es una !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 20: 100%|██████████| 103/103 [00:28<00:00,  3.63it/s, loss=3.143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Love, probably!\n",
      "TARGET: Seguramente su alegría tendrá por causa el amor.\n",
      "PREDICTED: – Pues bien , sí ...\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"I do.\"\n",
      "TARGET: -Sí.\n",
      "PREDICTED: - Sí .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 21: 100%|██████████| 103/103 [00:28<00:00,  3.62it/s, loss=3.131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: May thy joys be all dreams,\n",
      "TARGET: en sueños tus pasatiempos,\n",
      "PREDICTED: Si tú te ,\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: We must cast round for another scent.\"\n",
      "TARGET: Hemos de seguir buscando.\n",
      "PREDICTED: No había más que un paso de hielo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 22: 100%|██████████| 103/103 [00:28<00:00,  3.65it/s, loss=3.071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"They repent!\" exclaimed the sailor, shrugging his shoulders.\n",
      "TARGET: –¡Arrepentirse! –exclamó el marino encogiéndose de hombros.\n",
      "PREDICTED: –¡ ! – exclamó el marino , encogiéndose de hombros .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'She says that?' he cried.\n",
      "TARGET: –¡Conque dice eso! –exclamó–.\n",
      "PREDICTED: –¡ Qué dice ! – exclamó .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 23: 100%|██████████| 103/103 [00:28<00:00,  3.68it/s, loss=2.948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: It was only too true.\n",
      "TARGET: Era cierto.\n",
      "PREDICTED: No era demasiado cosa .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And pray why do you put your admiral to death?\"\n",
      "TARGET: --¿Y porqué han muerto á ese almirante?\n",
      "PREDICTED: Y ahora , ¡ qué la muerte !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 24: 100%|██████████| 103/103 [00:28<00:00,  3.65it/s, loss=2.920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: See how he is throwing out air and water through his blowers.\"\n",
      "TARGET: ¡Mira el aire y el agua que arroja por las narices!\n",
      "PREDICTED: ¿ Cómo sabe usted que el Sherlock Holmes se llama el agua ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I don't act and I worry.\n",
      "TARGET: Yo, al no hacer nada, me atormento.\n",
      "PREDICTED: No me pregunto si me .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 25: 100%|██████████| 103/103 [00:28<00:00,  3.60it/s, loss=2.859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Cruel, cruel deserter!\n",
      "TARGET: -¡Qué cruel fuiste, Jane!\n",
      "PREDICTED: -¡ Cruel , !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"But what hour is it, then?\" the Canadian asked.\n",
      "TARGET: -Pero ¿qué hora es? -preguntó el canadiense.\n",
      "PREDICTED: - Pero , ¿ qué es eso , el canadiense ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 26: 100%|██████████| 103/103 [00:28<00:00,  3.66it/s, loss=2.763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Still, jealousy is a strange transformer of characters.\n",
      "TARGET: ––No obstante, los celos pueden provocar extraños cambios en el carácter.\n",
      "PREDICTED: Sin embargo , hay celos , su situación está muy satisfecho .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Anyhow, he never got tallow-stains from a gas-jet.\n",
      "TARGET: En cualquier caso, un aplique de gas no produce manchas de sebo.\n",
      "PREDICTED: Más que no nos da un asunto para el .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 27: 100%|██████████| 103/103 [00:28<00:00,  3.64it/s, loss=2.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"We will make bellows of them!\"\n",
      "TARGET: Haremos de ellas fuelles de fragua.\n",
      "PREDICTED: ¡ Nos !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Ah! It is you, Treville.\n",
      "TARGET: ¡Ay sois vos, Tréville!\n",
      "PREDICTED: ¡ Ah ! ¡ Sois vos , Tréville !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 28: 100%|██████████| 103/103 [00:28<00:00,  3.67it/s, loss=2.613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"’Judge not rashly’, says the Gospel,\" replied the cardinal.\n",
      "TARGET: ¡No juzguéis temerariamente!, dice el Evangelio replicó el cardenal.\n",
      "PREDICTED: ¡ no debe ser la razón ! dijo el cardenal .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Had he said too much?\n",
      "TARGET: ¿Habría hablado demasiado?\n",
      "PREDICTED: ¿ Había comprendido así ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 29: 100%|██████████| 103/103 [00:28<00:00,  3.67it/s, loss=2.569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: What do you deduce from that?\"\n",
      "TARGET: ¿Qué deduce usted de eso?\n",
      "PREDICTED: ¿ Qué sabéis lo de eso ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Was there a ship at our disposal in some underground harbour?\n",
      "TARGET: ¿Había fondeado un buque en algún puerto interior?\n",
      "PREDICTED: ¿ Había algún buque a su viaje sobre sus papeles ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 30: 100%|██████████| 103/103 [00:28<00:00,  3.64it/s, loss=2.503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: If the current was interrupted, the magnet immediately became unmagnetized.\n",
      "TARGET: Si la corriente se interrumpía, el electroimán se desimantaba inmediatamente.\n",
      "PREDICTED: Si la corriente , Ned , se hallaba profundamente delante de él .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: His finger pulled the trigger before he had taken aim.\n",
      "TARGET: Su dedo oprimía el gatillo antes de apuntar bien.\n",
      "PREDICTED: El dedo sacó una seña del cadáver .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 31: 100%|██████████| 103/103 [00:28<00:00,  3.63it/s, loss=2.390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Papa says I mustn't.\n",
      "TARGET: ¿Has visto a Basilio Lukich?\n",
      "PREDICTED: papá me parece que no .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Our General stood in need of new recruits of young German Jesuits.\n",
      "TARGET: El padre general necesitaba una leva de jesuitas alemanes mozos.\n",
      "PREDICTED: El vapor continuaba cerca de esa nueva Ferrars y bajo el impasible .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 32: 100%|██████████| 103/103 [00:28<00:00,  3.67it/s, loss=2.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: CHAPTER XXVI\n",
      "TARGET: Capítulo XXVI\n",
      "PREDICTED: XXVI\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: That day the usual work was accomplished with even greater energy.\n",
      "TARGET: Aquel día se realizó con más vigor aún el trabajo habitual.\n",
      "PREDICTED: Sin día la día se también con el trabajo fría ; me parecía .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 33: 100%|██████████| 103/103 [00:27<00:00,  3.70it/s, loss=2.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"No. His orders were to stay in the house.\"\n",
      "TARGET: ––No, sus órdenes son permanecer en la casa.\n",
      "PREDICTED: - No , su órdenes cuando esperaba a la casa .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Was it Ayrton?\n",
      "TARGET: ¿Era Ayrton?\n",
      "PREDICTED: ¿ Era Ayrton ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 34: 100%|██████████| 103/103 [00:28<00:00,  3.66it/s, loss=2.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Me! why to me?\"\n",
      "TARGET: ¿A mí? ¿Y eso por qué?\n",
      "PREDICTED: ¿ A mí ? ¿ A qué me ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"It is a wound that confines him to his bed?\"\n",
      "TARGET: Entonces, ¿es una estocada lo que le retiene en su cama?\n",
      "PREDICTED: ¿ Es un herida que le haya venido a la cama ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 35: 100%|██████████| 103/103 [00:28<00:00,  3.65it/s, loss=2.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Mr. Conseil put one over on me!\"\n",
      "TARGET: El señor Conseil me estaba tomando el pelo.\n",
      "PREDICTED: -¡ Conseil ! - exclamó Conseil .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: His whole being was concentrated in this last word.\n",
      "TARGET: En esa frase estaba expresado todo el arponero.\n",
      "PREDICTED: Toda su situación estaba concentrada en el último .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 36: 100%|██████████| 103/103 [00:28<00:00,  3.66it/s, loss=2.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: IN WHICH THE CAPTIVE STILL CONTINUES HIS ADVENTURES\n",
      "TARGET: Donde todavía prosigue el cautivo su suceso\n",
      "PREDICTED: Donde el cautivo las maravillas del cautivo\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"I will not give him that trouble,\" I answered.\n",
      "TARGET: -No tiene por qué molestarse tanto -dije-.\n",
      "PREDICTED: –– No me da cuenta de esto –– respondí ––.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 37: 100%|██████████| 103/103 [00:27<00:00,  3.72it/s, loss=2.135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"No--stop!\" interrupted Colonel Dent.\n",
      "TARGET: -¡No! -interrumpió el coronel Dent-.\n",
      "PREDICTED: - No , no - respondió el coronel Dent -.\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Fix, seated in the bow, gave himself up to meditation.\n",
      "TARGET: Fix estaba meditabundo en la proa.\n",
      "PREDICTED: Fix , pues , bajó la campanilla y me dirigió a una especie .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 38: 100%|██████████| 103/103 [00:28<00:00,  3.67it/s, loss=2.115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: However, do as you please!'\n",
      "TARGET: Sin embargo, haz lo que te parezca mejor.\n",
      "PREDICTED: En fin ; como al fin te lo ruego .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He positively avoids me.\"\n",
      "TARGET: Parece evitarme.\n",
      "PREDICTED: Pronto tomó mi camarote .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 39: 100%|██████████| 103/103 [00:28<00:00,  3.66it/s, loss=2.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And you too, Mr. Spilett, you will eat some!\"\n",
      "TARGET: –Con mucho gusto –dijo el corresponsal–.\n",
      "PREDICTED: – Y tú , señor Spilett , con nosotros .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'I ought to have done that long ago.'\n",
      "TARGET: ¡Ya podíamos haberlo hecho hace tiempo!\n",
      "PREDICTED: Hace tiempo que me he pasado ya .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 40: 100%|██████████| 103/103 [00:28<00:00,  3.63it/s, loss=2.043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Vronsky remained silent.\n",
      "TARGET: Vronsky callaba.\n",
      "PREDICTED: Vronsky callaba .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Certainly, Neb,\" answered Cyrus Harding.\n",
      "TARGET: –Sí, Nab –repuso Ciro Smith.\n",
      "PREDICTED: – Sí , señor – contestó Ciro Smith –.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 41: 100%|██████████| 103/103 [00:29<00:00,  3.53it/s, loss=1.978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: M Bonacieux was at his door.\n",
      "TARGET: El señor Bonacieux estaba a su puerta.\n",
      "PREDICTED: El señor Bonacieux estaba en la puerta .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The man replied that the Count had gone to the stables.\n",
      "TARGET: El criado contestó que el Conde se dirigía a las cuadras\n",
      "PREDICTED: El hombre respondió que el señor conde iban juntos .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 42: 100%|██████████| 103/103 [00:30<00:00,  3.41it/s, loss=1.883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And who has abandoned you--is that it?\"\n",
      "TARGET: Y que os ha abandonado, ¿no es eso?\n",
      "PREDICTED: -¿ Y quién tiene usted dinero ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I have brought my other son and daughter to see you.\n",
      "TARGET: He traído a mi otro hijo e hija para que se conozcan.\n",
      "PREDICTED: He traído mi hijo y a mi hijo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 43: 100%|██████████| 103/103 [00:30<00:00,  3.37it/s, loss=1.894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: It was worth while going a little hungry.\n",
      "TARGET: Vale la pena quedarse sin comer.\n",
      "PREDICTED: Su dolor pasaba un poco gigantesco .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: — Oui, je sais ce que vous avez vu.\n",
      "TARGET: ––Sí, sé ya todo lo que usted vio.\n",
      "PREDICTED: –– Le aseguro que no es verdad .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 44: 100%|██████████| 103/103 [00:30<00:00,  3.39it/s, loss=1.864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Take away the prisoner,\" said the commissary to the two guards.\n",
      "TARGET: Llevaos al prisionero dijo el comisario a los dos guardias.\n",
      "PREDICTED: Tomad a vuestra prisionera dijo el comisario al comisario .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Milady smiled.\n",
      "TARGET: Milady sonrió.\n",
      "PREDICTED: Milady sonrió .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 45: 100%|██████████| 103/103 [00:30<00:00,  3.40it/s, loss=1.857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Let us be off to Tver.\n",
      "TARGET: La ventana está abierta.\n",
      "PREDICTED: Hemos ido con la suficiente .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I am simply unhappy.\n",
      "TARGET: Soy muy desgraciada.\n",
      "PREDICTED: Estoy verdaderamente desgraciada .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 46: 100%|██████████| 103/103 [00:29<00:00,  3.47it/s, loss=1.801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: You are killing me now.\"\n",
      "TARGET: Ya me estás matando ahora.\n",
      "PREDICTED: Aquí estás aquí .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Not exactly. Your witch's skill is rather at fault sometimes.\"\n",
      "TARGET: No es eso precisamente.\n",
      "PREDICTED: Ni más claramente el almirante de visitar a las opiniones de su doble manera .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 47: 100%|██████████| 103/103 [00:28<00:00,  3.67it/s, loss=1.816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: How clearly he puts everything!'\n",
      "TARGET: Lo ve todo con una claridad...\n",
      "PREDICTED: ¡ Qué delgada y qué se lo pone !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"You are hungry,\" I remarked.\n",
      "TARGET: ––Viene usted hambriento ––comenté.\n",
      "PREDICTED: - Usted es - dije .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 48: 100%|██████████| 103/103 [00:28<00:00,  3.64it/s, loss=1.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: It was forty-eight hours since I had taken any nourishment.\n",
      "TARGET: Cuando me desperté, una nueva mesa estaba servida.\n",
      "PREDICTED: Era cuarenta y ocho horas que no hubieran hecho más o cinco años .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He entered the chamber and closed the door behind him.\n",
      "TARGET: Entró en la habitación y cerró la puerta tras sí.\n",
      "PREDICTED: Entró en la habitación y la puerta salió de la puerta .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 49: 100%|██████████| 103/103 [00:28<00:00,  3.55it/s, loss=1.745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Levin felt guilty but could do nothing.\n",
      "TARGET: Levin, sin poderlo remediar, se sentía culpable.\n",
      "PREDICTED: Levin sentía culpable y no podía hacer nada .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: CHAPTER XVIII\n",
      "TARGET: Capítulo XVIII\n",
      "PREDICTED: XVIII\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 50: 100%|██████████| 103/103 [00:30<00:00,  3.39it/s, loss=1.725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Kitty, don't be angry!\n",
      "TARGET: –Kitty, no te enfades.\n",
      "PREDICTED: –¿ Cómo , Kitty ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'What's the matter?'\n",
      "TARGET: –¿Qué te pasa?\n",
      "PREDICTED: –¿ Qué te pasa ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 51: 100%|██████████| 103/103 [00:29<00:00,  3.50it/s, loss=1.700]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I am simply unhappy.\n",
      "TARGET: Soy muy desgraciada.\n",
      "PREDICTED: Estoy incapaz .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: What will he say when he returns?\"\n",
      "TARGET: ¿Qué dirá cuando vuelva?\n",
      "PREDICTED: ¿ Qué opina usted ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 52: 100%|██████████| 103/103 [00:28<00:00,  3.66it/s, loss=1.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"What do you mean, then?\"\n",
      "TARGET: -Entonces, ¿a qué se refiere?\n",
      "PREDICTED: ––¿ Qué quiere usted decir ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"No!\" replies my uncle.\n",
      "TARGET: —¡No! —responde mi tío.\n",
      "PREDICTED: — No , mi tío .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 53: 100%|██████████| 103/103 [00:28<00:00,  3.68it/s, loss=1.711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"How!\" said Bonacieux, astonished.\n",
      "TARGET: ¿Cómo? dijo Bonacieux, extrañado.\n",
      "PREDICTED: ¿ Cómo ? dijo Bonacieux asombrado .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He liked Katavasov because of his clear and simple outlook on life.\n",
      "TARGET: Katavasov le atraía por la claridad y sencillez de sus ideas.\n",
      "PREDICTED: Se acercó a Katavasov y tranquila , y tranquila ; pero es excelente vida .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 54: 100%|██████████| 103/103 [00:28<00:00,  3.64it/s, loss=1.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"I have heard of you, Mr. Holmes.\n",
      "TARGET: ––He oído hablar de usted, señor Holmes.\n",
      "PREDICTED: - Lo he oído , señor Holmes .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"You are very good.\n",
      "TARGET: -Es usted muy amable.\n",
      "PREDICTED: ––¿ Cómo está usted bien ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 55: 100%|██████████| 103/103 [00:28<00:00,  3.64it/s, loss=1.657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Yes, sir.\"\n",
      "TARGET: -Sí.\n",
      "PREDICTED: - Sí , señor .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Jane Eyre\n",
      "TARGET: Jane Eyre\n",
      "PREDICTED: Jane ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 56: 100%|██████████| 103/103 [00:28<00:00,  3.63it/s, loss=1.649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: How the theories will hinder us, won't they?\"\n",
      "TARGET: ¡Cuánto van a darnos que hacer!\n",
      "PREDICTED: ¿ Cómo los gatos son estos gatos ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Es ist ein ganz einfaches Ding,' [Oh, yes!\n",
      "TARGET: Es ist ein ganz einfaches Ding.\n",
      "PREDICTED: – Es preciso – añadió Vronsky .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 57: 100%|██████████| 103/103 [00:27<00:00,  3.69it/s, loss=1.634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: One part of Captain Nemo's secret life had been unveiled.\n",
      "TARGET: Eso desvelaba una parte de la misteriosa existencia del capitán Nemo.\n",
      "PREDICTED: Un poco de tierra , el capitán Nemo había sido alguna .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Round the world?\" cried Fix.\n",
      "TARGET: ¿La vuelta al mundo? Exclamó Fix.\n",
      "PREDICTED: ¿ La vuelta al mundo ? dijo Fix .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 58: 100%|██████████| 103/103 [00:27<00:00,  3.68it/s, loss=1.627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The company rose to go into the garden.\n",
      "TARGET: Los invitados se levantaron en aquel momento para salir al jardín.\n",
      "PREDICTED: El choque tomó bastante al jardín .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Kindly follow this man.\"\n",
      "TARGET: Tengan la amabilidad de seguir a este hombre.\n",
      "PREDICTED: - Entonces , esta persona .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 59: 100%|██████████| 103/103 [00:28<00:00,  3.60it/s, loss=1.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: In your place I would stake the furniture against the horse.\"\n",
      "TARGET: En vuestro lugar, yo jugaría vuestros arneses contra vuestro caballo.\n",
      "PREDICTED: En vuestro sitio les sigo roto el caballo .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Why don't you like her husband?\n",
      "TARGET: –¿Y por qué a su marido no?\n",
      "PREDICTED: –¿ Cómo no te gusta su marido ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 60: 100%|██████████| 103/103 [00:28<00:00,  3.63it/s, loss=1.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"As far as the centre of the earth, Axel.\"\n",
      "TARGET: —Sí, hasta el centro de la tierra.\n",
      "PREDICTED: — Tan sólo el centro de la tierra .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: His finger pulled the trigger before he had taken aim.\n",
      "TARGET: Su dedo oprimía el gatillo antes de apuntar bien.\n",
      "PREDICTED: Sus manos se tras haber marchado como el horizonte .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 61: 100%|██████████| 103/103 [00:27<00:00,  3.69it/s, loss=1.590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I inquired soon if he had not been to London.\n",
      "TARGET: Le pregunté si había estado en Londres.\n",
      "PREDICTED: Pregunté por ti si no se hubiera dicho nada .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Yes, when the house where he fraternizes is suspected.\"\n",
      "TARGET: Sí, cuando la casa en la que confraterniza con ese amigo es sospechosa.\n",
      "PREDICTED: - Sí , cuando el resto de la casa .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 62: 100%|██████████| 103/103 [00:27<00:00,  3.69it/s, loss=1.608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Why _did_ Mr. Rochester enforce this concealment?\n",
      "TARGET: ¿Por qué Rochester toleraba aquello?\n",
      "PREDICTED: ¿ Y qué hizo entonces los turcos ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'He is pitiable, he is overwhelmed with remorse...'\n",
      "TARGET: Es digno también de compasión; el arrepentimiento le tiene abatido.\n",
      "PREDICTED: Se trata de un paseo como cerrado por remordimientos .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 63: 100%|██████████| 103/103 [00:28<00:00,  3.62it/s, loss=1.580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Drink and relate, then.\"\n",
      "TARGET: Bebed y contad.\n",
      "PREDICTED: - Bebe y maquinalmente .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The atmosphere was turning white and milky.\n",
      "TARGET: La atmósfera estaba blanca, lechosa.\n",
      "PREDICTED: Hacía una escena que salieron bajo los planes y la capa de espesor .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 64: 100%|██████████| 103/103 [00:28<00:00,  3.65it/s, loss=1.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'But what was there in church on Sunday?\n",
      "TARGET: –¿Qué pasó el domingo en la iglesia? –preguntó el Príncipe–.\n",
      "PREDICTED: – Pero ¿ qué era en una iglesia ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Was our presence on board perhaps a burden to him?\n",
      "TARGET: ¿Tal vez se le hacía insoportable nuestra presencia a bordo?\n",
      "PREDICTED: ¿ Era nuestra presencia un detalle como yo ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 65: 100%|██████████| 103/103 [00:28<00:00,  3.68it/s, loss=1.558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Everybody knew your errand.\"\n",
      "TARGET: Todos lo saben.\n",
      "PREDICTED: Todos lo demás se que es capaz de enamorarse .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I didn't tell my two companions about this new danger.\n",
      "TARGET: Me abstuve de comunicar este nuevo peligro a mis dos compañeros.\n",
      "PREDICTED: No pude responderle entre mis dos compañeros es éste .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 66: 100%|██████████| 103/103 [00:27<00:00,  3.68it/s, loss=1.573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And you said nothing?\"\n",
      "TARGET: -¿Y no dijo nada?\n",
      "PREDICTED: ¿ Y no habéis dicho nada ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: During the last week of the month of August the weather moderated again.\n",
      "TARGET: Durante la última semana de aquel mes de agosto el tiempo cambió de nuevo.\n",
      "PREDICTED: Durante la semana de las siete de roca desnuda de .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 67: 100%|██████████| 103/103 [00:27<00:00,  3.70it/s, loss=1.545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"What about master's live babirusa?\"\n",
      "TARGET: -¿Y el babirusa vivo del señor?\n",
      "PREDICTED: -¿ Y el señor de los \" No es allá ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I may then tell the cardinal, with respect to this little woman--\"\n",
      "TARGET: Puedo, por tanto, decir al cardenal que, respecto a esa mujer...\n",
      "PREDICTED: Sé que el cardenal le a su mujer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 68: 100%|██████████| 103/103 [00:27<00:00,  3.70it/s, loss=1.554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Nothing.\"\n",
      "TARGET: –¡Sí!...\n",
      "PREDICTED: –– Nada .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Then she broke the silence to cry out,\n",
      "TARGET: Entonces ella rompió el silencio para exclamar:\n",
      "PREDICTED: Luego , dio el silencio de cabeza un instante , cruzó la mente .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 69: 100%|██████████| 103/103 [00:27<00:00,  3.70it/s, loss=1.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I am staying there while I conduct the inquiry.\"\n",
      "TARGET: Clair. Me estoy alojando allí mientras llevo a cabo la investigación.\n",
      "PREDICTED: aquí por buen camino ; supongo que la investigación .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"When will we reach Vanikoro?\"\n",
      "TARGET: -¿Cuándo estaremos en Vanikoro?\n",
      "PREDICTED: -¿ Cuándo nos vamos a Vanikoro ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 70: 100%|██████████| 103/103 [00:27<00:00,  3.68it/s, loss=1.538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"I have won five pistoles of Aramis.\"\n",
      "TARGET: Le he ganado cinco pistolas a Aramis.\n",
      "PREDICTED: Tengo cinco pistolas .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Signed, Richard Mason.'\"\n",
      "TARGET: Firmado: Richard Mason.»\n",
      "PREDICTED: : ¡ Cómo te llamas !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 71: 100%|██████████| 103/103 [00:28<00:00,  3.68it/s, loss=1.555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: That happened which M. de Treville had foreseen.\n",
      "TARGET: Lo que había previsto el señor de Tréville ocurrió.\n",
      "PREDICTED: De maravilla que la misma del señor Tréville había contado encima .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'I never maintained it,' thought Levin...\n",
      "TARGET: «Jamás lo he asegurado», pensó Levin.\n",
      "PREDICTED: «¡ No he pensado nada ! – comentó Levin –.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 72: 100%|██████████| 103/103 [00:27<00:00,  3.70it/s, loss=1.530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"The Miss Reeds could not play as well!\" said she exultingly.\n",
      "TARGET: -¡Las señoritas no tocan tan bien! -dijo con entusiasmo-.\n",
      "PREDICTED: -¡ La señorita Stoner permitía desplazarse como no ! - dijo con voz baja .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Our case is not complete.\n",
      "TARGET: -Nuestro caso no está terminado.\n",
      "PREDICTED: El caso no es absolutamente nada .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 73: 100%|██████████| 103/103 [00:27<00:00,  3.70it/s, loss=1.535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: No wind, and not a cloud in the sky.\n",
      "TARGET: No hay viento, ni se ve una nube en el cielo.\n",
      "PREDICTED: El viento y media absoluta ventana estaba vacía y los colonos .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: You should come and join us here.'\n",
      "TARGET: Porque en este caso podría sentarse con nosotros.\n",
      "PREDICTED: Usted volverá a tu parte .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 74: 100%|██████████| 103/103 [00:27<00:00,  3.74it/s, loss=1.541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Gregor had almost entirely stopped eating.\n",
      "TARGET: Gregorio ya no comía casi nada.\n",
      "PREDICTED: Gregorio todos estaban únicamente para tomar siquiera .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Well, shall we go?'\n",
      "TARGET: ¿Vamos?\n",
      "PREDICTED: –¿ Qué ? ¿ Vamos ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 75: 100%|██████████| 103/103 [00:27<00:00,  3.70it/s, loss=1.531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Yes, yes, yes...'\n",
      "TARGET: –Sí, sí, sí.\n",
      "PREDICTED: – Sí , sí ...\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"But suppose it is an extinct volcano?\"\n",
      "TARGET: —¿Y si se trata de un cráter apagado?\n",
      "PREDICTED: –¿ Y eso está cerrado ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 76: 100%|██████████| 103/103 [00:27<00:00,  3.69it/s, loss=1.519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'I shall be very glad,' replied Koznyshev, still smiling.\n",
      "TARGET: –Conforme. Me gustará mucho –contestó Sergio Ivanovich, siempre sonriente.\n",
      "PREDICTED: – Lo haré muy simpática – dijo Sergio , sonriendo .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: This current swept along with it a whole host of moving creatures.\n",
      "TARGET: La corriente arrastraba con ella a todo un mundo de seres vivos.\n",
      "PREDICTED: Esta corriente , se por una persona una verdadera debilidad de nuestro viaje .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 77: 100%|██████████| 103/103 [00:27<00:00,  3.70it/s, loss=1.527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Have I anything to pay?\" demanded d’Artagnan.\n",
      "TARGET: ¿Debo algo? preguntó D'Artagnan.\n",
      "PREDICTED: ¿ Es algo que hacer ? preguntó D ' Artagnan .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Certainly, my best.\n",
      "TARGET: -Sí, querida.\n",
      "PREDICTED: - Desde luego , sería lo mejor .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 78: 100%|██████████| 103/103 [00:27<00:00,  3.70it/s, loss=1.510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Oh, yes, yes; you are right.\n",
      "TARGET: ¡Oh, sí, sí, tenéis razón!\n",
      "PREDICTED: ¡ Oh , sí ! ¿ Conque tenéis razón ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: One is a she-bear.\n",
      "TARGET: ¡Vámonos ahora mismo a Tver!\n",
      "PREDICTED: Hay una .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 79: 100%|██████████| 103/103 [00:27<00:00,  3.71it/s, loss=1.504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But what kind of man was he?\n",
      "TARGET: Pero, ¿qué clase de persona era?\n",
      "PREDICTED: Pero ¿ qué hombre era él ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"No, I am descending.\"\n",
      "TARGET: ––En efecto, voy descendiendo.\n",
      "PREDICTED: - No , me quedo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 80: 100%|██████████| 103/103 [00:27<00:00,  3.69it/s, loss=1.504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Certainly, my best.\n",
      "TARGET: -Sí, querida.\n",
      "PREDICTED: - Desde luego , es lo que necesito .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The Nautilus kept descending.\n",
      "TARGET: El Nautilus continuó descendiendo.\n",
      "PREDICTED: El Nautilus parecía un violento .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 81: 100%|██████████| 103/103 [00:28<00:00,  3.66it/s, loss=1.500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Her arms rose and her hands dropped on his shoulders.\n",
      "TARGET: Sus manos se levantaron y se posaron en los hombros de Levin.\n",
      "PREDICTED: Ana se levantó y cogió la mano sobre sus hombros .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Why so?\" said Sancho.\n",
      "TARGET: -Pues, ¿por qué? -dijo Sancho.\n",
      "PREDICTED: -¿ Por qué ? - dijo Sancho -.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 82: 100%|██████████| 103/103 [00:27<00:00,  3.69it/s, loss=1.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I am staying there while I conduct the inquiry.\"\n",
      "TARGET: Clair. Me estoy alojando allí mientras llevo a cabo la investigación.\n",
      "PREDICTED: Estoy aquí para mi propia roca . Yo estoy vivo en los medios .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"With a wood round it?\"\n",
      "TARGET: -¿Y un bosque alrededor?\n",
      "PREDICTED: -¿ Con un bosque ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 83: 100%|██████████| 103/103 [00:27<00:00,  3.68it/s, loss=1.511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He said he would come back.'\n",
      "TARGET: Ha dicho que volvería.\n",
      "PREDICTED: Él dijo que había llegado .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Why don't you consult my art?\"\n",
      "TARGET: -¿Cómo no quería consultar mi ciencia?\n",
      "PREDICTED: -¿ Cómo usted ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 84: 100%|██████████| 103/103 [00:27<00:00,  3.77it/s, loss=1.488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"That is the exact situation.\n",
      "TARGET: -Buenos días, Holmes -dijo el baronet-.\n",
      "PREDICTED: Es la situación de la situación .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"And does she go from Suez directly to Bombay?\"\n",
      "TARGET: ¿Y de Suez se marcha directamente a Bombay?\n",
      "PREDICTED: ¿ Y está bien en Suez de este Bombay ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 85: 100%|██████████| 103/103 [00:27<00:00,  3.79it/s, loss=1.500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Lord!\n",
      "TARGET: -¡Por Dios, Anne! -exclamó su hermana-.\n",
      "PREDICTED: ¡ Ay , Dios !\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"The devil!\" muttered Passepartout.\n",
      "TARGET: ¡Diantre! exclamó Picaporte.\n",
      "PREDICTED: ¡ Diablos ! exclamó Picaporte .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 86: 100%|██████████| 103/103 [00:27<00:00,  3.81it/s, loss=1.484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Don't you see now whence these words have been taken?\"\n",
      "TARGET: ¿Ve usted ahora de dónde se han tomado esas palabras?\n",
      "PREDICTED: ¿ No va usted francamente de esos palabras ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Yes, perfectly well--intimately even.\"\n",
      "TARGET: Sí, perfectamente, mucho incluso.\n",
      "PREDICTED: – Sí , en efecto : el está íntimamente ligado .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 87: 100%|██████████| 103/103 [00:27<00:00,  3.81it/s, loss=1.487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: To this no reply was possible.\n",
      "TARGET: No había respuesta posible a esa pregunta.\n",
      "PREDICTED: A ello no fue posible .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: You consider Vronsky an aristocrat. I don't.\n",
      "TARGET: Tú consideras que Vronsky es un aristócrata y yo no.\n",
      "PREDICTED: Usted ha hecho Vronsky junto a Vronsky .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 88: 100%|██████████| 103/103 [00:27<00:00,  3.79it/s, loss=1.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: They fear them, therefore they must know them.\"\n",
      "TARGET: Los temen, luego los conocen.\n",
      "PREDICTED: No están más pues , como pueden saber .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Does he wish it?'\n",
      "TARGET: ¿Lo desea él?\n",
      "PREDICTED: ¿ Estará usted satisfecho ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 89: 100%|██████████| 103/103 [00:27<00:00,  3.78it/s, loss=1.495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: My undaunted uncle calmly shook his head.\n",
      "TARGET: Mi tío sacudió la cabeza con calma.\n",
      "PREDICTED: Mi tío golpeó la cabeza con la cabeza .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Where?\n",
      "TARGET: ''¿Adónde?\n",
      "PREDICTED: Pero ¿ dónde ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 90: 100%|██████████| 103/103 [00:27<00:00,  3.79it/s, loss=1.479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: How can I ask them away from her?\"\n",
      "TARGET: ¿Cómo puedo pedirles que la dejen?\n",
      "PREDICTED: ¿ Cómo podré retener de ella ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Gräuben was far away; and I never hoped to see her again.\n",
      "TARGET: Y ni aun esperanzas tenía de volver a verla jamás.\n",
      "PREDICTED: Graüben .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 91: 100%|██████████| 103/103 [00:26<00:00,  3.82it/s, loss=1.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"The Coroner: What do you mean?\n",
      "TARGET: »El juez: ¿Qué quiere decir con eso?\n",
      "PREDICTED: » El juez : ¿ Qué queréis decir ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Amen!' from the invisible choir, again floated through the air.\n",
      "TARGET: «¡Amén!» llenaron de nuevo el aire las voces del coro.\n",
      "PREDICTED: – Amén a Amén de paseo por los Scherbazky , se dirigió al aire .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 92: 100%|██████████| 103/103 [00:27<00:00,  3.80it/s, loss=1.486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Karenin entered the boudoir.\n",
      "TARGET: Alexey Alejandrovich entró en el gabinete de Ana.\n",
      "PREDICTED: Alexey Alejandrovich entró con los últimos movimientos .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: CHAPTER 32\n",
      "TARGET: CAPITULO XXXII\n",
      "PREDICTED: Capítulo XXXII\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 93: 100%|██████████| 103/103 [00:27<00:00,  3.77it/s, loss=1.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: CHAPTER XXVIII. THE RESCUE IN THE WHISPERING GALLERY\n",
      "TARGET: Capítulo XXVIII\n",
      "PREDICTED: Capítulo XXVIII\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Very ill?\n",
      "TARGET: ¿Muy enferma?\n",
      "PREDICTED: – Muy mal .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 94: 100%|██████████| 103/103 [00:27<00:00,  3.78it/s, loss=1.473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"That is well.\n",
      "TARGET: ––Eso está bien.\n",
      "PREDICTED: –– Estupendo .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Speak out, man, and don't stand staring!\"\n",
      "TARGET: ¡Hable, caramba, y no se me quede mirando!\n",
      "PREDICTED: ¡ Hablad , hombre , y no con voz alta !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 95: 100%|██████████| 103/103 [00:27<00:00,  3.77it/s, loss=1.479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Confound it!\" exclaimed the sailor.\n",
      "TARGET: –¡Maldición! –exclamó el marino, sin contenerse.\n",
      "PREDICTED: –¡ Miss Eyre ! – exclamó el marino .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"I am, then, your prisoner?\"\n",
      "TARGET: ¿Soy, pues, vuestra prisionera?\n",
      "PREDICTED: Entonces , ¿ os haré vuestra prisionera ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 96: 100%|██████████| 103/103 [00:27<00:00,  3.79it/s, loss=1.477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Alexander smiled gaily.\n",
      "TARGET: Alejandro Vronsky, que lo sabía, sonrió con jovialidad.\n",
      "PREDICTED: Alejandro .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He was a nobleman, a man equal to Buckingham in every respect.\n",
      "TARGET: Era un gran señor, era un hombre en todo el igual de Buckingham.\n",
      "PREDICTED: El noble lo comprenderá todo a su comandante Ketty .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 97: 100%|██████████| 103/103 [00:27<00:00,  3.78it/s, loss=1.475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Impatient of delay, with reckless pace\n",
      "TARGET: Salió el deseo de compás, y el paso\n",
      "PREDICTED: Cruel Vireno , con gran estruendo , , se dirigieron a sus pies .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: CHAPTER VII\n",
      "TARGET: VII\n",
      "PREDICTED: VII\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 98: 100%|██████████| 103/103 [00:26<00:00,  3.83it/s, loss=1.470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"What do you mean?\"\n",
      "TARGET: -¿Por qué?\n",
      "PREDICTED: -¿ Qué quieres decir ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Being fossils, we looked upon all those things as mere jokes.\n",
      "TARGET: En nuestra calidad de fósiles, nos burlábamos de estas maravillas inútiles.\n",
      "PREDICTED: No pudiendo nosotros , los miró de una situación , sobre las regiones .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 99: 100%|██████████| 103/103 [00:27<00:00,  3.79it/s, loss=1.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'I was only going to say that...'\n",
      "TARGET: –Quisiera decirte...\n",
      "PREDICTED: – Sólo quería decir ..\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"What, when we have nothing to do but keep going down!\"\n",
      "TARGET: —¡Cómo fatigoso, cuando siempre caminamos cuesta abajo!\n",
      "PREDICTED: -¡ Qué , cuándo no vamos a conseguir nada !\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore') # Filtering warnings\n",
    "config = get_config() # Retrieving config settings\n",
    "train_model(config) # Training model with the config arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taiko\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¿Qué, cuando no tenemos nada que hacer sino seguir bajando!', 'Sólo iba a decir que...', 'Siendo fósiles, veíamos todas esas cosas como meras bromas.', '¿Qué quieres decir?']\n"
     ]
    }
   ],
   "source": [
    "src_text = ['What, when we have nothing to do but keep going down!', 'I was only going to say that...', 'Being fossils, we looked upon all those things as mere jokes.', 'What do you mean?']\n",
    "inputs = tokenizer(src_text, return_tensors='pt', padding=True)\n",
    "translated = model.generate(**inputs)\n",
    "tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "Transformer From Scratch With PyTorch🔥",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
