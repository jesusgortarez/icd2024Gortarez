{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrucciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construir una representaci√≥n basada en Bag of Words. Se deber√° leer el archivo de texto (ver adjunto) que contiene m√∫ltiples entradas de texto y dos clases distintas. El vector de caracter√≠sticas final debe tener como √∫ltima columna el atributo de clase.\n",
    "Saludos,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalaci√≥n necesaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install spacy\n",
    "#%pip install nltk\n",
    "#%pip install sklearn\n",
    "#%python -m spacy download es_core_news_sm\n",
    "#%spacy download es_core_news_sm\n",
    "\n",
    "#Si usas anaconda abre la terminal de tu Environments y ejecuta las lineas\n",
    "#python -m spacy download es_core_news_sm\n",
    "#spacy download es_core_news_sm\n",
    "#Reiniciar terminal y continuar (ejecutando todo de nuevo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importaci√≥n de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy as spc\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# cargar modelo de spaCy para espa√±ol necesario para la lematizacion\n",
    "nlp = spc.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jesus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Jesus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jesus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar stopwords\n",
    "spanish_stopwords = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    label                                               text\n",
      "0       1  ESAS COSAS Y OTRAS PUEDEN PASAR POR MANTENER A...\n",
      "1       1  28: te amodio, odio a la perra de tu amiga ‚ò∫Ô∏è‚ò∫...\n",
      "2       1  @LaDivinaDiva Callate maldita perra. O seguro ...\n",
      "3       1  @MarysabelPuerto Mejor callate cara de puta o ...\n",
      "4       1  @xarita327 @TRIKYHUMOR @yonier2012 @casTa1326 ...\n",
      "5       1              @CocotePR @ashleyhonohan callate puta\n",
      "6       1  Y el inmigrante recibe ayuda del rico Estado l...\n",
      "7       1  De los moros no se puede esperar nada bueno, y...\n",
      "8       1  ¬øPor que si a una mujer le pegan un tiro en la...\n",
      "9       1  Analicemos esto: ¬øSi te pones unos shorts as√≠,...\n",
      "10      0  \"See... Tal vez les recordo como Peron protegi...\n",
      "11      0  PIETRAPIERCE STORY: PURS SANGS ARABES STARS DE...\n",
      "12      0  ¬øQu√© dice este de frivolizar el acoso escolar?...\n",
      "13      0  #Suiza üá®üá≠ retira el permiso a 189 refugiados q...\n",
      "14      0  Hoy quiero denunciaaaaaaar A LA GENTE PUTO GUA...\n",
      "15      0  Redomicilie su sociedad offshore en Emiratos √Å...\n",
      "16      0  @hermanntertsch Basta! Poned pie en pared a ta...\n",
      "17      0  SEMANA DE LA JUVENTUD. TORNEO FUTBOL7  / FUTBO...\n",
      "18      0  @lNeko_ @xTiko98 Callate y metete party de una...\n",
      "19      0  Cu√°ntos inmigrantes creemos que hay, y cu√°ntos...\n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo CSV\n",
    "file_path = 'df_mini_HS.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funci√≥n de limpieza de oraciono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_oraciono(oracion):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                u\"\\U0001F600-\\U0001F64F\"  # Emoticonos\n",
    "                                u\"\\U0001F300-\\U0001F5FF\"  # S√≠mbolos y pictogramas\n",
    "                                u\"\\U0001F680-\\U0001F6FF\"  # Transporte y s√≠mbolos\n",
    "                                u\"\\U0001F700-\\U0001F77F\"  # S√≠mbolos adicionales\n",
    "                                u\"\\U0001F780-\\U0001F7FF\"  # Geometr√≠a adicional\n",
    "                                u\"\\U0001F800-\\U0001F8FF\"  # Suplemento de pictogramas\n",
    "                                u\"\\U0001F900-\\U0001F9FF\"  # Pictogramas adicionales\n",
    "                                u\"\\U0001FA00-\\U0001FA6F\"  # S√≠mbolos adicionales\n",
    "                                u\"\\U00002702-\\U000027B0\"  # Otros s√≠mbolos (corazones, flechas)\n",
    "                                u\"\\U000024C2-\\U0001F251\"  # Otros s√≠mbolos adicionales\n",
    "                                \"]+\", flags=re.UNICODE)\n",
    "    oracion = emoji_pattern.sub(r'', oracion)  # Eliminar emojis\n",
    "    oracion = oracion.lower()\n",
    "    oracion = re.sub(r\"@\\S+\", \"\", oracion)  # Eliminar menciones a usuarios\n",
    "    oracion = re.sub(r\"http[s]?\\://\\S+\", \"\", oracion)  # Eliminar enlaces \n",
    "    oracion = re.sub(r\"#\\S+\", \"\", oracion)  # Eliminar hashtags\n",
    "    oracion = re.sub(r\"[0-9]\", \"\", oracion)  # Eliminar n√∫meros\n",
    "    oracion = re.sub(r\"(\\(.*\\))|(\\[.*\\])\", \"\", oracion)  # Eliminar par√©ntesis y corchetes\n",
    "    oracion = re.sub(r\"\\n\", \"\", oracion)  # Eliminar caracteres de nueva l√≠nea\n",
    "    oracion = re.sub(r\"(http[s]?\\://\\S+)|([\\[\\(].*[\\)\\]])|([#@]\\S+)|\\n\", \"\", oracion)  # Eliminar varios patrones\n",
    "    oracion = re.sub(r\"(\\.)|(,)\", \"\", oracion)  # Eliminar puntos y comas\n",
    "    oracion = re.sub(r\"[¬°!]\", \"\", oracion)  # Eliminar signos de admiraci√≥n \n",
    "    oracion = re.sub(r\"[¬ø?]\", \"\", oracion)  # Eliminar signos de exclamaci√≥n\n",
    "    oracion = re.sub(r\"[-\\:\\/\\\"\\'\\*\\`\\s]+\", \" \", oracion).strip() #Eliminar varios caracteres\n",
    "\n",
    "    return oracion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funci√≥n para tokenizar, remover stopwords y lematizar con spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesamiento_oracion(oracion):\n",
    "    oracion = limpiar_oraciono(oracion)\n",
    "    tokens = word_tokenize(oracion)\n",
    "    palabras_filtradas = [palabra for palabra in tokens if palabra not in spanish_stopwords]\n",
    "    lema = nlp(\" \".join(palabras_filtradas))\n",
    "    oracion_procesada = \" \".join([token.lemma_ for token in lema])\n",
    "\n",
    "    return oracion_procesada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: ['acoso' 'agar' 'agresi√≥n' 'amigo' 'amodio' 'analicer' 'aprieto' 'arab'\n",
      " 'arar' 'as√≠' 'ayuda' 'bala' 'bastar' 'bueno' 'bus' 'cabeza' 'cadete'\n",
      " 'callate' 'calle' 'camello' 'cara' 'categoria' 'cerebro' 'chavista'\n",
      " 'chiste' 'chorizo' 'ciento' 'ciudadano' 'colaborador' 'colau' 'conto'\n",
      " 'cortad' 'cosa' 'creer' 'cuatro' 'cu√°nto' 'c√°llate' 'decir' 'dejar'\n",
      " 'denunciaaaaaaar' 'des' 'detras' 'dia' 'donbenitensar' 'durar' 'eichmann'\n",
      " 'elite' 'emirato' 'encontrar' 'equipo' 'escolar' 'espa√±a' 'esperar'\n",
      " 'esperas' 'expulsar' 'falta' 'favor' 'femenino' 'festivit' 'frivolizar'\n",
      " 'fuenlabrado' 'futbol' 'garcho' 'gente' 'guarra' 'hacer' 'hijo' 'hoy'\n",
      " 'humillaci√≥n' 'ilegal' 'inmigracion' 'inmigrante' 'inscrito'\n",
      " 'josewifakers' 'juventud' 'kiev' 'ko' 'ladr√≥n' 'llamado' 'luchado'\n",
      " 'lugar' 'madre' 'maldito' 'mantener' 'mantero' 'marico' 'mata' 'medio'\n",
      " 'mejor' 'metete' 'mientras' 'minato' 'morir' 'moro' 'mujer' 'mundo'\n",
      " 'nacional' 'nazi' 'negrata' 'nota' 'odio' 'offshore' 'oler' 'olvidar'\n",
      " 'on' 'pared' 'party' 'pasar' 'pa√≠s' 'peguir' 'permiso' 'peron' 'perra'\n",
      " 'perrir' 'pie' 'pietrapiercir' 'pintado' 'pobre' 'poder' 'poned' 'poner'\n",
      " 'propio' 'protegio' 'provocaci√≥n' 'purs' 'puta' 'puto' 'p√≥n' 'quer√©is'\n",
      " 'quiero' 'rayo' 'realidad' 'recibir' 'recordo' 'redomicilie' 'refugiado'\n",
      " 'reputa' 'retirar' 'rico' 'sang' 'see' 'seguro' 'semana' 'semejante'\n",
      " 'shame' 'shorts' 'si' 'sociedad' 'stars' 'story' 'subir' 'sudor' 'tal'\n",
      " 'tanto' 'tijerazo' 'tiro' 'tol' 'tonter√≠a' 'torneo' 'tres' 'turista'\n",
      " 'unido' 'user' 'vacaci√≥n' 'verdad' 'vez' 'vida' 'viola' 'voolka' 'v√≠a'\n",
      " 'yogur√≠n' 'you' '√°rabe' '√©l']\n"
     ]
    }
   ],
   "source": [
    "# Aplica la funci√≥n de procesamiento a cada oraci√≥n del archivo y crea una nueva columna \"oracion_procesada\"\n",
    "df['oracion_procesada'] = df['text'].apply(procesamiento_oracion)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectores = vectorizer.fit_transform(df['oracion_procesada'])\n",
    "vocabulario = vectorizer.get_feature_names_out()\n",
    "print(f\"Vocabulario:\", vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oraci√≥n de entrada [1]: ESAS COSAS Y OTRAS PUEDEN PASAR POR MANTENER A LA INMIGRACION ILEGAL EN ESPA√ëA Y NO EXPULSARLOS ¬øNO? - La agresi√≥n de los manteros a un turista pone a Colau en un aprieto https://t.co/C7mZWXAl9P v√≠a @Elperiodico\n",
      "Oraci√≥n lematizada [1]: cosa poder pasar mantener inmigracion ilegal espa√±a expulsar √©l agresi√≥n mantero turista poner colau aprieto v√≠a\n",
      "Vectores Bag of Words [1]: [[0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1]]\n",
      "Oraci√≥n de entrada [2]: 28: te amodio, odio a la perra de tu amiga ‚ò∫Ô∏è‚ò∫Ô∏èpero t√∫ eres mi vida *tijerazo*\n",
      "Oraci√≥n lematizada [2]: amodio odio perra amigo vida tijerazo\n",
      "Vectores Bag of Words [2]: [[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n",
      "Oraci√≥n de entrada [3]: @LaDivinaDiva Callate maldita perra. O seguro eres un pobre marico detras de un user femenino. Chavista colaborador\n",
      "Oraci√≥n lematizada [3]: callate maldito perra seguro pobre marico detras user femenino chavista colaborador\n",
      "Vectores Bag of Words [3]: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Oraci√≥n de entrada [4]: @MarysabelPuerto Mejor callate cara de puta o reputa como tu madre.. Se nota que te hacen falta estos https://t.co/3jeehNQzy1\n",
      "Oraci√≥n lematizada [4]: mejor callate cara puta reputa madre nota hacer falta\n",
      "Vectores Bag of Words [4]: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Oraci√≥n de entrada [5]: @xarita327 @TRIKYHUMOR @yonier2012 @casTa1326 @LizMontoyapan30 @El_SuperRaton @changodepravado C√°llate puta!!\n",
      "Oraci√≥n lematizada [5]: c√°llate puto\n",
      "Vectores Bag of Words [5]: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Oraci√≥n de entrada [6]: @CocotePR @ashleyhonohan callate puta\n",
      "Oraci√≥n lematizada [6]: callate puto\n",
      "Vectores Bag of Words [6]: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Oraci√≥n de entrada [7]: Y el inmigrante recibe ayuda del rico Estado ladr√≥n, que se olvida de los nacionales, mientras nos viola y mata. Pintada con una verdad a medias. https://t.co/4icBlhB3j9\n",
      "Oraci√≥n lematizada [7]: inmigrante recibir ayuda rico ladr√≥n olvidar nacional mientras viola mata pintado verdad medio\n",
      "Vectores Bag of Words [7]: [[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0]]\n",
      "Oraci√≥n de entrada [8]: De los moros no se puede esperar nada bueno, y esto te lo dicen los propios √°rabes. Que tambi√©n han luchado contra ellos.  #StopInvasion #StopIslam #Closeborders #Openborders. https://t.co/MfmBlBK55I\n",
      "Oraci√≥n lematizada [8]: moro poder esperar bueno decir propio √°rabe luchado\n",
      "Vectores Bag of Words [8]: [[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]]\n",
      "Oraci√≥n de entrada [9]: ¬øPor que si a una mujer le pegan un tiro en la cabeza dura tres dias en morirse? Porque a los tres dias la bala encuentra el cerebro.\n",
      "Oraci√≥n lematizada [9]: si mujer peguir tiro cabeza durar tres dia morir √©l tres dia bala encontrar cerebro\n",
      "Vectores Bag of Words [9]: [[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 2 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "Oraci√≥n de entrada [10]: Analicemos esto: ¬øSi te pones unos shorts as√≠, en la calle, ¬øqu√© esperas que te digan? ¬øAcoso? ¬øO Provocaci√≥n... https://t.co/vRDyNzMdCG\n",
      "Oraci√≥n lematizada [10]: analicer si p√≥n shorts as√≠ calle esperas decir acoso provocaci√≥n\n",
      "Vectores Bag of Words [10]: [[1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Oraci√≥n de entrada [11]: \"See... Tal vez les recordo como Peron protegio a Eichmann y cientos de nazis. O les conto a los arabes \"\"El chiste del araba que se lo garcho un camello\"\" https://t.co/AKonmuHKn0\"\n",
      "Oraci√≥n lematizada [11]: see tal vez recordo peron protegio eichmann ciento nazi conto arab chiste arar garcho camello\n",
      "Vectores Bag of Words [11]: [[0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]]\n",
      "Oraci√≥n de entrada [12]: PIETRAPIERCE STORY: PURS SANGS ARABES STARS DES FESTIVITES A AGAR EL M... https://t.co/G2HgYnflxg\n",
      "Oraci√≥n lematizada [12]: pietrapiercir story purs sang arab stars des festivit agar m\n",
      "Vectores Bag of Words [12]: [[0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Oraci√≥n de entrada [13]: ¬øQu√© dice este de frivolizar el acoso escolar? ¬øPor favor quer√©is dejar de decir semejantes tonter√≠as? https://t.co/ndVjx73SH3\n",
      "Oraci√≥n lematizada [13]: decir frivolizar acoso escolar favor quer√©is dejar decir semejante tonter√≠a\n",
      "Vectores Bag of Words [13]: [[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 2 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Oraci√≥n de entrada [14]: #Suiza üá®üá≠ retira el permiso a 189 refugiados que fueron de vacaciones a sus pa√≠ses   https://t.co/CLtwIle75v v√≠a @abc_es\n",
      "Oraci√≥n lematizada [14]: retirar permiso refugiado vacaci√≥n pa√≠s v√≠a\n",
      "Vectores Bag of Words [14]: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0]]\n",
      "Oraci√≥n de entrada [15]: Hoy quiero denunciaaaaaaar A LA GENTE PUTO GUARRA QUE HUELE A SUDOR Y CHORIZO Y SE SUBE AL BUS DEJANDO A TOL MUNDO KO. Shame on you ü§¨\n",
      "Oraci√≥n lematizada [15]: hoy quiero denunciaaaaaaar gente puto guarra oler sudor chorizo subir bus dejar tol mundo ko shame on you\n",
      "Vectores Bag of Words [15]: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]]\n",
      "Oraci√≥n de entrada [16]: Redomicilie su sociedad offshore en Emiratos √Årabes Unidos. https://t.co/gJYLHarqZA\n",
      "Oraci√≥n lematizada [16]: redomicilie sociedad offshore emirato √°rabe unido\n",
      "Vectores Bag of Words [16]: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0]]\n",
      "Oraci√≥n de entrada [17]: @hermanntertsch Basta! Poned pie en pared a tanta provocaci√≥n y cortad la humillaci√≥n de estos cuatro hijos de perra,\n",
      "Oraci√≥n lematizada [17]: bastar poned pie pared tanto provocaci√≥n cortad humillaci√≥n cuatro hijo perrir\n",
      "Vectores Bag of Words [17]: [[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Oraci√≥n de entrada [18]: SEMANA DE LA JUVENTUD. TORNEO FUTBOL7  / FUTBOL 5 CATEGORIA CADETE EQUIPOS INSCRITOS: LOS YOGURINES LA ELITE RAYO DONBENITENSE LOS NEGRATAS FUENLABRADA MINATO DE KIEV LOS JOSEWIFAKERS VOOLKA... https://t.co/JQ3DiaXtbb\n",
      "Oraci√≥n lematizada [18]: semana juventud torneo futbol futbol categoria cadete equipo inscrito yogur√≠n elite rayo donbenitensar negrata fuenlabrado minato kiev josewifakers voolka\n",
      "Vectores Bag of Words [18]: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 0\n",
      "  1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0]]\n",
      "Oraci√≥n de entrada [19]: @lNeko_ @xTiko98 Callate y metete party de una puta vez\n",
      "Oraci√≥n lematizada [19]: callate metete party puta vez\n",
      "Vectores Bag of Words [19]: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]]\n",
      "Oraci√≥n de entrada [20]: Cu√°ntos inmigrantes creemos que hay, y cu√°ntos hay en realidad. Ciudadanos de un lugar llamado mundo... https://t.co/ngyCsVJjBp\n",
      "Oraci√≥n lematizada [20]: cu√°nto inmigrante creer cu√°nto realidad ciudadano lugar llamado mundo\n",
      "Vectores Bag of Words [20]: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 2\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# itaracion para todas las oraciones para imprimir resultados\n",
    "for i, row in df.iterrows():\n",
    "    oracion = row['text']\n",
    "    oracion_lematizada = row['oracion_procesada']\n",
    "    \n",
    "    print(f\"Oraci√≥n de entrada [{i+1}]:\", oracion)\n",
    "    print(f\"Oraci√≥n lematizada [{i+1}]:\", oracion_lematizada)\n",
    "    print(f\"Vectores Bag of Words [{i+1}]:\", vectores[i].toarray())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acoso</th>\n",
       "      <th>agar</th>\n",
       "      <th>agresi√≥n</th>\n",
       "      <th>amigo</th>\n",
       "      <th>amodio</th>\n",
       "      <th>analicer</th>\n",
       "      <th>aprieto</th>\n",
       "      <th>arab</th>\n",
       "      <th>arar</th>\n",
       "      <th>as√≠</th>\n",
       "      <th>...</th>\n",
       "      <th>vez</th>\n",
       "      <th>vida</th>\n",
       "      <th>viola</th>\n",
       "      <th>voolka</th>\n",
       "      <th>v√≠a</th>\n",
       "      <th>yogur√≠n</th>\n",
       "      <th>you</th>\n",
       "      <th>√°rabe</th>\n",
       "      <th>√©l</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   acoso  agar  agresi√≥n  amigo  amodio  analicer  aprieto  arab  arar  as√≠  \\\n",
       "0      0     0         1      0       0         0        1     0     0    0   \n",
       "1      0     0         0      1       1         0        0     0     0    0   \n",
       "2      0     0         0      0       0         0        0     0     0    0   \n",
       "3      0     0         0      0       0         0        0     0     0    0   \n",
       "4      0     0         0      0       0         0        0     0     0    0   \n",
       "\n",
       "   ...  vez  vida  viola  voolka  v√≠a  yogur√≠n  you  √°rabe  √©l  class  \n",
       "0  ...    0     0      0       0    1        0    0      0   1      1  \n",
       "1  ...    0     1      0       0    0        0    0      0   0      1  \n",
       "2  ...    0     0      0       0    0        0    0      0   0      1  \n",
       "3  ...    0     0      0       0    0        0    0      0   0      1  \n",
       "4  ...    0     0      0       0    0        0    0      0   0      1  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertir la matriz BoW a un DataFrame y agregar la clase como √∫ltima columna\n",
    "df_bw = pd.DataFrame.sparse.from_spmatrix(vectores,columns = vocabulario)\n",
    "df_bw['class'] = df['label'].values\n",
    "df_bw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien realizar una bolsa de palabras (Bag of Words) es relativamente sencillo, la complejidad aumenta cuando los datos de entrada se vuelven menos comprensibles.\n",
    "\n",
    "En esta tarea, por ejemplo, un filtro complejo y necesario ser√≠a eliminar todas las palabras que no pertenecen al idioma espa√±ol. Sin embargo, si se hace utilizando un diccionario, esto podr√≠a eliminar palabras como \"amodio\", que aunque poco comunes, son correctas. Otro problema es el uso del espa√±ol antiguo, el cual tendr√≠a que ser normalizado; por ejemplo, \"cortad\" deber√≠a transformarse en \"cortar\". Adem√°s, existen palabras sin sentido aparente que, en realidad, podr√≠an ser simplemente errores tipogr√°ficos, y eliminarlas podr√≠a hacernos perder informaci√≥n valiosa. Esto nos llevar√≠a a realizar una tarea similar a la de los correctores ortogr√°ficos como el de Google, donde corregir√≠amos errores y sugerir√≠amos alternativas con algo como \"quiz√°s quiso decir\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ser√≠a bueno aclarar el prop√≥sito del Bag of Words y qu√© nivel de eliminaci√≥n de palabras es adecuado para realizar una limpieza adecuada. Tambi√©n, ser√≠a √∫til conocer el objetivo de la tarea, de modo que el estudiante pueda decidir hasta qu√© punto llevar la limpieza "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algunas preguntas interesantes que se pueden investigar a futuro son:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¬øRealmente es necesario borrar los emojis, considerando que al final la Bag of Words no los toma en cuenta?\n",
    "2. Otros caracteres que no se consideran en el proceso son: `-`, `/`, `:`, ``, `''`, `\"`, `*`. ¬øEs necesario filtrarlos?\n",
    "3. ¬øPor qu√© a veces la lematizaci√≥n cambia el g√©nero de las palabras, como en el caso de \"puta\" a \"puto\", mientras que en otras ocasiones s√≠ respeta el g√©nero, dejando \"puta\"?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
